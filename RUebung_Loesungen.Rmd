---
title: "R-Übungen - Lösungen"
subtitle: "Modul „Statistische Aspekte der Analyse molekularbiologischer und genetischer Daten“"
author: "Janne Pott"
date: "WS 2019/20"
output: pdf_document
---
![](GraphikA.jpg)
\newpage 
\setcounter{tocdepth}{2}
\tableofcontents 
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
options(knitr.table.format = "latex")
options(knitr.kable.NA = "")
```

# Vorwort

In diesem R Markdown Dokument sind alle Lösungen der Übungsaufgaben des Semester enthalten. 

Folgende R-Pakete und Programme sind für die Bearbeitung nötig bzw. sinnvoll:

```{r initialise, echo=FALSE, warning=FALSE, message=FALSE}
setwd("/net/ifs1/san_projekte/projekte/genstat/13_lehre/WS1920_Statistik/uebung/RUebungen/")
.libPaths("/net/ifs1/san_projekte/projekte/genstat/07_programme/rpackages/amanMRO/")

# Hier sollen alle notwendigen Pakete stehen die im Laufe der Uebung genutzt werden
library(knitr)
library(data.table)
library(lubridate)
library(MASS)
library(nlme)
library(ggplot2)
library(meta)
library(qqman)
library(ivpack)
library(MendelianRandomization)

plink_call<-"/net/ifs1/san_projekte/projekte/genstat/07_programme/plink1.9/vs180103_stable_beta_51/unix64/plink"
```

```{r dummy, eval=F}
setwd("D:/Lehre/WS1920_Statistik/uebung/RUebungen/")
.libPaths("C:/Program Files/R/R-3.6.0/library")

library(knitr)
library(data.table)
library(plyr)
library(MASS)
library(nlme)
library(ggplot2)
library(meta)
library(qqman)
library(ivpack)
library(MendelianRandomization)

plink_call<-"C:/Program Files/plink1.9/vs180103_stable_beta_51/unix64/plink"
```

Im Rahmen dieser Übung können nicht alle in der Vorlesung behandelte Methoden besprochen werden. Wichtige Pakete für Genexpression sind z.B.

* **sva**: Surrogate Variable Analysis --> z.B. für Batchadjustierung von Genexpressions- bzw. Metabolomdaten

* **lumi**: BeadArray Specific Methods for Illumina Methylation and Expression Microarrays --> z.B. für Präprozessierung von Genexpressionsdaten

* **limma**: Linear Models for Microarray Data --> z.B. für TWAS

* **MatrixEQTL**: Ultra Fast eQTL Analysis via Large Matrix Operations --> z.B. für eQTL-Analysen

Die Pakete **sva**, **lumi** und **limma** sind über den [Bioconductor](https://bioconductor.org/) abrufbar, **MatrixEQTL** klassisch über CRAN. 

Bei Fragen oder Problemen gibt viele nützliche [Cheat-Sheets](https://rstudio.com/resources/cheatsheets/) zu R-Tools und viele Hilfeseiten (z.B. stackoverflow.com). Ein wichtiges Nachschlagwerk für Graphiken in R ist das [Cookbook](http://www.cookbook-r.com/Graphs/). 

\newpage 

# Blatt 1: Grundlagen von R

Termin: 29.10.2019

Abgabe: Alle Aufgaben

## Aufgabe 1: R als Taschenrechner

Berechnen Sie folgende Terme: 

a) $|3^5 - 2^{10}|$

b) $sin(\frac34 \pi)$

c) $\frac{16!}{5!11!}$

d) $\sqrt{37-8} + \sqrt{11}$

e) $e^{-2.7}/0.1$

f) $2.3^8 + \ln(7.4) - \tan(0.3\pi)$

g) $\log_{10}(27)$

h) $\ln(\pi)$

i) $\ln(-1)$

### Lösung

```{r B1A1, eval=T,echo=T}
abs(3^5 - 2^10)
sin((3/4)*pi)
factorial(16)/(factorial(5)*factorial(11))
sqrt(37-8) + sqrt(11)
exp(-2.7)/0.1
2.3^8 + log(7.4) - tan(0.3*pi)
log10(27)
log(pi)
log(-1)
```

## Aufgabe 2: Variablen und Folgen

Erzeugen Sie folgende $a_1, ..., a_{10}$:

a) $a_n = 3^n$

b) $a_n = e^{-n}$

c) $a_n = (1 + \frac1n)^n$

d) $a_n = \sin(n \frac{\pi}{10})$

### Lösung

```{r B1A2, eval=T,echo=T}
n<-seq(1:10)
a<-3^n
a
b<-exp(-n)
b
c<-(1 + 1/n)^n
c
d<-sin(n*pi/10)
d
```

## Aufgabe 3: Funktionen

a) Definieren Sie die Funktion $h(x)=\sin(\sqrt{x})$ und werten Sie sie an den Stellen 0, 0.1, 0.2, ..., 0.9, und 1 aus. 

b) Definieren Sie die Funktion $g(x,y) = \sqrt{x^2 + y^2}$ und werten Sie sie für $x=3$ und $y=7$ aus.

c) Erstellen Sie von beiden Funktionen je einen Plot (für $h(x)$ den Bereich $0 \leq x \leq 2$ bzw. für $g(x,y)$ $-2 \leq x, y \leq 2$). 

### Lösung

```{r B1A3, eval=T,echo=T}
h<-function(x){sin(sqrt(x))}
x<-seq(0,1,0.1)
h(x)

g<-function(x,y){sqrt(x^2 + y^2)}
g(3,7)

curve(h,0,2)

X=Y=seq(-2,2,length=50)
Z=outer(X,Y,g)
persp(X,Y,Z,theta = 30,phi = 30,d=100)
image(X,Y,Z);contour(X,Y,Z,add=T)
```

## Aufgabe 4: Vektoren und Matrizen

a) Erzeugen Sie einen Vektor *A* mit den Quadratzahlen 1, 4, 9, ..., 400 als Einträgen. 

b) Bilden Sie zwei Vektoren *B* und *C* aus den ersten bzw. letzten zehn Einträngen von *A*. Erzeugen Sie daraus einen Vektor *D* mit 50 Einträgen, in dem zunächst einmal die Elemente von *A*, zweimal die von *C* und einmal die von *B* auftreten.

c) Erzeugen Sie aus dem Vektor *D* die 10x5 Matrix *M*. 

### Lösung

```{r B1A4, eval=T,echo=T}
n<-c(1:20)
n
A<-n^2
A

B<-A[1:10]
C<-A[11:20]
D<-c(A,C,C,B)
D

M<-matrix(D,nrow = 10)
M
```

## Aufgabe 5: Dateineingabe

a) Laden Sie den Datensatz **iris**. 

b) Ändern Sie die Klasse von *data.frame* zu *data.table*.

c) Wie viele Einträge sind pro Spezies vorhanden? 

d) Wie lang und breit sind im Mittel die Blätter pro Spezie? Nutzen Sie dazu die Funktion *lapply()*.

e) Definieren Sie eine neue Spalte als Produkt der Kelchblattlänge und -breite. 

f) Wie groß ist die mittlere Differenz der Blattlänge (Kelch - Blüte) in der Spezies *setosa*?

### Lösung

```{r B1A5, eval=T,echo=T}
data(iris)
head(iris)

getDTthreads()
setDTthreads(1)

setDT(iris)

iris[,.N,Species]

iris[,lapply(.SD,mean),Species]

iris[,test := Sepal.Length*Sepal.Width]
head(iris)

iris[Species=="setosa",mean(Sepal.Length - Petal.Length)]
iris[,mean(Sepal.Length - Petal.Length),Species]
```

```{r B1_clear, echo=F}
rm(list = setdiff(ls(), "plink_call"))
```

\newpage 

# Blatt 2: Deskriptive Statistiken in R

Termin: 12.11.2019

Abgabe: Multiples Testen (Nachtrag)

## Nachtrag: Multiples Testen

In der letzen Theorie-Übung konnte die letzte Präsenzaufgabe nicht mehr bearbeitet werden. Diese wird daher hier wiederholt / ergänzt. 

Es wurden 9 SNPs auf Assoziation mit Krankheit X getestet. Die daraus resultierenden p-Werte sind in der untenstehenden Tabelle festgehalten (Table 1). Die Ergebnisse wurden zunächst zum Signifikanzniveau von 5% ausgewertet, was das multiple Testen nicht berücksichtigte.

a) Definieren Sie die Begriffe false discovery rate (FDR) und family-wise error rate (FWER).

b) Bestimmen Sie die jeweiligen Schranken zu den drei genannten Verfahren in R, indem Sie geeignete Schleifen erstellen. Welche SNPs sind nach Korrektur immer noch signifikant mit Krankheit X assoziiert? Füllen Sie die untenstehende Tabelle (Table 1) geeignet aus.

c) Was kontrolliert welches Verfahren (FDR oder FWER)?

d) Sind diese Methoden in R schon implementiert? Falls ja, wenden Sie diese Funktionen an, und vergleichen diese mit ihren eigenen Ergebnissen.

```{r B2N1, eval=T,echo=F}
pvec<-c(0.023,0.006,0.025,0.35,0.3,0.04,0.2,0.002,0.015)
pid<-c("rs1001","rs1002","rs1003","rs1004","rs1005",
       "rs1006","rs1007","rs1008","rs1009")
dumDat<-data.frame(pid,pvec,NA,NA,NA,NA,NA,NA)
options(knitr.kable.NA = "")
knitr::kable(dumDat, position = "!b",
             caption = "Auszufüllende Tabelle zum Nachtrag (Blatt 2). 
                        B=Bonferroni, BonH=Bonferroni-Holm, 
                        BenH=Benjamini-Hochberg",
             col.names = c("SNP","p-Wert","Schranke B","adj p B",
                           "Schranke BonH","adj. p BonH",
                           "Schranke BenH","adj. p BenH"))
```

### Lösung a)
FP:= *false positives*, wahre $H_0$ abgelehnt

TP:= *true positives*, falsche $H_0$ abgelehnt

Die **FWER** ist die Wahrscheinlichkeit, dass Anzahl FP >0 (mindestens eine von allen untersuchten Nullhypothesen wurde fälschlich ablehnt). 
**Ziel**: #FP möglichst klein 

Die **FDR** ist der Erwartungswert von FP/(FP+TP) (Anteil der fälschlich abgelehnten an allen abgelehnten Hypothesen)
**Ziel**: Anteil an FP fix

### Lösung b) & c)
```{r B2N2, echo=T,eval=T}
pvec<-c(0.023,0.006,0.025,0.35,0.3,0.04,0.2,0.002,0.015)
pid<-c("rs1001","rs1002","rs1003","rs1004","rs1005",
       "rs1006","rs1007","rs1008","rs1009")
myDat<-data.frame(pid,pvec)

p_bonferroni<-0.05/9
p_bonferroni
myDat$p_bonferroni<-round(p_bonferroni,4)
myDat[myDat$pvec<p_bonferroni,]

ordering<-order(myDat$pvec,decreasing = F)
myDat<-myDat[ordering,]
p_bon_holm<-c()
for(i in 1:9){
  p_bon_holm[i]<-0.05/(10-i)
}
myDat$p_bon_holm<-round(p_bon_holm,4)
myDat[myDat$pvec<p_bon_holm,]

p_ben_hoch<-c()
for(i in 1:9){
  p_ben_hoch[i]<-0.05/9*i
}
myDat$p_ben_hoch<-round(p_ben_hoch,4)
filt<-myDat$pvec<=myDat$p_ben_hoch
x<-max(grep(T,filt))
myDat[1:x,]
```

**Bonferroni-Verfahren**: kontrolliert FWER, nur SNP rs1008 ist nach Korrektur signifikant mit X assoziiert. 

**Bonferroni-Holm-Verfahren**: kontrolliert FWER, SNPs rs1008 und rs1002 sind nach Korrektur signifikant mit X assoziiert.

**Benjamini-Hochberg-Verfahren**: kontrolliert FDR, SNPs rs1008, rs1002, rs1009, rs1001 und rs1003 sind nach Korrektur signifikant mit X assoziiert. 

### Lösung d)
Mittels der Funktion *p.adjust()* können die adjustierten p-Werte bestimmt werden. 

```{r B2N3, echo=T,eval=T}
myDat$p.adj.1<-round(p.adjust(myDat$pvec,method = "bonferroni"),4)
myDat$p.adj.2<-round(p.adjust(myDat$pvec,method = "holm"),4)
myDat$p.adj.3<-round(p.adjust(myDat$pvec,method = "BH"),4)
knitr::kable(myDat[,c(1,2,3,6,4,7,5,8)], 
             caption = "Ausgefüllte Tabelle zum Nachtrag (Blatt 2). 
                        B=Bonferroni, BonH=Bonferroni-Holm, 
                        BenH=Benjamini-Hochberg",
             col.names = c("SNP","p-Wert","Schranke B","adj p B",
                           "Schranke BonH","adj. p BonH",
                           "Schranke BenH","adj. p BenH"))
```
Erklärung anhand des Bonferroni-Verfahrens: statt zu prüfen, ob $p<0.05/n$ ist, wird der p-Wert auf die $n$ Tests adjustiert, $p_{adj} = p*n$ und dann zu $0.05$ getestet. 

Hierbei kann es vorkommen, dass $p_{adj}>1$ wird, was per Definition eines p-Wertes nicht möglich ist. In diesem Fall wird der adjustierte p-Wert auf 1 gesetzt. 

Bei Bonferroni-Holm und Benjamini-Hochberg wird die Reihenfolge beachtet: wenn ein adjustierter p-Wert kleiner ist als der vorherige wird dieser auf den kleineren Wert gesetzt (SNPs rs1001, rs1003 und rs1009). 

\newpage

## Aufgabe 1: Deskriptive Statistik

Laden Sie den Datensatz **ergometer.RData** in R wie folgt ein:

```{r B2A1_0, echo=T,eval=F}
loaded1<-load("../data/ergometer.RData")
loaded1
```
Eine Parameterbeschreibung ist in Table 3 gegeben. 

a)  Schlagen Sie nach, was der Befehl *get()* bewirkt und wenden Sie ihn geeignet auf **myDat** an! In welcher Situation kann dieser Befehl sehr wichtig sein?

b)	Berechnen Sie *Alter* und *BMI* der Probanden. Dazu können Sie zum Beispiel die Funktion *mdy()* aus dam Paket **lubridate** verwenden. 

c)	Bestimmen Sie deskriptive Statistiken für die Größen *ergometer*, *lactate*, *BMI* und *Alter* für Männer und Frauen getrennt. Fertigen Sie zusätzlich Histogramme an.

d)	Vergleichen Sie *ergometer* zwischen den Geschlechtern unter Verwendung eines geeigneten Tests.

e)	Korrelieren Sie *ergometer* mit *lactate*, *BMI* und *Alter*.

```{r B2A1_1, echo=F,eval=T}
rm(list = setdiff(ls(), "plink_call"))
loaded1<-load("../data/ergometer.RData")
loaded1
var<-colnames(myDat)
dummy1<-c("Durchlaufende ID-Nummer","Geschlecht","Geburtstag","Erhebungsdatum",
         "Größe","Gewicht","Leistung im Ergometer","Milchsäure im Blut")
dummy2<-c(NA,"1 = Mann; 2 = Frau","Monat/Tag/Jahr","Monat/Tag/Jahr",
          "in m","in kg","in Watt/kg","in mg/dl")
dumTab<-data.frame(var,dummy1,dummy2)
knitr::kable(dumTab, position = "!b",
             caption = "Parameterbeschreibung zu Aufgabe 1 (Blatt 2)",
             col.names = c("Variable","Beschreibung","Codierung / Einheit"))
```

### Lösung a)
Der Befehl *get()* gibt den Wert eines benannten Objekts zurück. Daten, die in Form eines .RData Objektes gespeichert wurden, wurden mit ihrem Variablennamen gespeichert und dementsprechend mit diesem Namen wieder eingelesen. Mittels *loaded1* wird der Name als Character gespeichert und kann ausgegeben werden. Wenn nun eine Umbennenung nötig ist, oder der Aufruf in einer Schleife erfolgt, kann der Datensatz einfach umbenannt werden. 

```{r r B2A1_2, echo=T,eval=T}
loaded1
myTab<-get(loaded1)
table(myTab==myDat)
```

### Lösung b)

```{r B2A1_3,eval=T}
date1<-mdy(myDat$Bday)
date2<-mdy(myDat$Tday)
setDT(myDat)
class(myDat)
colnames(myDat)
myDat[,alter:=round(as.numeric((date2-date1)/365.25), 2)]
myDat[,BMI:=round(weight/height^2,2)]
```

### Lösung c)

```{r B2A1_4,eval=T}
# Deskriptive Statistiken
tab1<-myDat[sex==1, sapply(.SD, summary),.SDcols=c("ergometer","lactate","alter","BMI")]
tab2<-myDat[sex==1, sapply(.SD, sd),.SDcols=c("ergometer","lactate","alter","BMI")]
tab3<-myDat[sex==1, sapply(.SD, var),.SDcols=c("ergometer","lactate","alter","BMI")]
tab<-rbind(tab1,tab2,tab3)
rownames(tab)[c(7,8)]<-c("SD","Var")
knitr::kable(tab, position = "!b",digits =3,
             caption = "Deskriptive Statistiken für die kontinuierlichen 
             Größen in Männern (Blatt 2, Aufgabe 1)",
             col.names = c("Ergometer","Laktat","Alter","BMI"))

tab1<-myDat[sex==2, sapply(.SD, summary),.SDcols=c("ergometer","lactate","alter","BMI")]
tab2<-myDat[sex==2, sapply(.SD, sd),.SDcols=c("ergometer","lactate","alter","BMI")]
tab3<-myDat[sex==2, sapply(.SD, var),.SDcols=c("ergometer","lactate","alter","BMI")]
tab<-rbind(tab1,tab2,tab3)
rownames(tab)[c(7,8)]<-c("SD","Var")
knitr::kable(tab, position = "!b",digits =3,
             caption = "Deskriptive Statistiken für die kontinuierlichen 
             Größen in Frauen (Blatt 2, Aufgabe 1)",
             col.names = c("Ergometer","Laktat","Alter","BMI"))

# Histogramme
par(mfrow = c(1,2))

qqnorm(myDat$ergometer,main = "Ergometer"); qqline(myDat$ergometer, col = 2)
hist(myDat$ergometer,breaks = 10,main = "Ergometer")
ks.test(myDat$ergometer,pnorm,mean=mean(myDat$ergometer),sd=sd(myDat$ergometer))

qqnorm(myDat$lactate,main = "Laktat"); qqline(myDat$lactate, col = 2)
hist(myDat$lactate,breaks = 10,main = "Laktat")
ks.test(myDat$lactate,pnorm,mean=mean(myDat$lactate),sd=sd(myDat$lactate))

qqnorm(myDat$alter,main = "Alter"); qqline(myDat$alter, col = 2)
hist(myDat$alter,breaks = 10,main = "Alter")
ks.test(myDat$alter,pnorm,mean=mean(myDat$alter),sd=sd(myDat$alter))

qqnorm(myDat$BMI,main = "BMI"); qqline(myDat$BMI, col = 2)
hist(myDat$BMI,breaks = 10,main = "BMI")
ks.test(myDat$BMI,pnorm,mean=mean(myDat$BMI),sd=sd(myDat$BMI))
```

*BMI* ist annähernd normalverteilt, *alter* und *laktat* nicht, und *ergometer* ist grenzwertig. Für die Analyse der Ergometerleistung kann entweder der t-Test (Annahme: normalverteilt) oder Mann-Whitney U Test (parameterfrei, Annahme: nicht normalverteilt). Für die Korrelationsanalyse wird der Spearmans Rangkorrelationskoeffizient berechnet. 

### Lösung d)
```{r B2A1_5,eval=T}
t.test(myDat$ergometer ~ myDat$sex) 
wilcox.test(myDat$ergometer ~ myDat$sex) 
```

Die Ergebnisse sind sich ähnlich: Die Nullhypothese, dass es keinen Unterschied zwischen Männern und Frauen gibt, kann nicht abgelehnt werden. 

```{r B2A1_6,eval=T}
# All credits to https://www.r-bloggers.com/more-on-exploring-correlations-in-r/
cor.prob <- function (X, dfr = nrow(X) - 2) {
  R <- cor(X, use="pairwise.complete.obs",method="spearman")
  above <- row(R) < col(R)
  r2 <- R[above]^2
  Fstat <- r2 * dfr/(1 - r2)
  R[above] <- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] <- NA
  R
}
cor.prob(myDat[,7:10])
```

Bei der Korrelation kann man im unteren Dreieck der Matrix Spearmans $\rho$ ablesen, im oberen Dreieck die p-Werte. Laktat und Alter sind mit Ergometer korreliert, BMI nicht. Das kann durchaus daran liegen, dass sowohl die Ergometerleistung als auch BMI das Körpergewicht berücksichtigen, was sich auf die Korrelation auswirkt. Wenn man das Gewicht wieder herausrechnet, sieht man eine Korrelation und einen signifikanten Zusammenhang zwischen Watt und Geschlecht. 

```{r B2A1_7,eval=T}
myDat[,watt:=ergometer*weight]
cor.test(myDat$watt,myDat$BMI,method = "spearman")  
t.test(myDat$watt ~ myDat$sex)
wilcox.test(myDat$watt ~ myDat$sex)
par(mfrow=c(1,2))
boxplot(myDat$ergometer ~ myDat$sex, main="Boxplot: Ergometer",
        xlab="Geschlecht",ylab="Watt/kg")
plot(myDat[,BMI],myDat[,ergometer],xlab="BMI",ylab="Watt/kg",main="Scatterplot",pch=19)
boxplot(myDat$watt ~ myDat$sex, main="Boxplot: Watt",
        xlab="Geschlecht",ylab="Watt")
plot(myDat[,BMI],myDat[,watt],xlab="BMI",ylab="Watt",main="Scatterplot",pch=19)
```

\newpage

## Aufgabe 2: Gepaarte Tests

Laden Sie den Datensatz **haendigkeit.RData** in R ein. Eine Parameterbeschreibung ist in Table 6 gegeben. 

```{r B2A2_0, echo=T,eval=F}
loaded2<-load("../data/haendigkeit.RData")
loaded2
```

a) Berechnen Sie geeignete deskriptive Statistiken für die Variablen *sex*, *WrHnd*, *NWHnd*, *WHnd*, *Fold*, *Clap*, und *height*!

b) Berechnen und visualisieren Sie den Längenunterschied von Schreib- und Nichtschreibhand. Es gibt einen Ausreißer. Filtern Sie diesen für weitere Analysen. 

c) Testen Sie, ob es Unterschiede in den Handlängen zwischen Männern und Frauen gibt. Konstruieren Sie Boxplots.

d) Testen Sie für Männer und Frauen getrennt, ob es Längenunterschiede zwischen Schreib- und Nichtschreibhand gibt.

e) Analysieren Sie die Beziehung zwischen Schreibhand, Armverschränkung und Klatschen, dabei die unentschiedenen Fälle filtern. 

f) Testen Sie für Männer und Frauen getrennt, ob es Beziehungen zwischen Größe, Länge der Hand und Unterschied zwischen Schreib-/Nichtschreibhand gibt. 

g) Führen Sie eine Regressionsanalyse des Unterschieds zwischen Schreib-/Nichtschreibhand für Frauen durch. Verwenden Sie dafür die am besten korrelierte Variable (siehe f). 

```{r B2A2_1, echo=F,eval=T}
rm(list = setdiff(ls(), "plink_call"))
loaded2<-load("../data/haendigkeit.RData")
loaded2
var<-colnames(myDat)
dummy1<-c("Durchlaufende ID-Nummer","Geschlecht","Länge der Schreibhand",
          "Länge der Nichtschreibhand","Schreibhand",
          "Präferenz für Armverschränkung","Präferenz für Klatschen","Größe")
dummy2<-c(NA,"1 = Mann; 2 = Frau","in cm","in cm",
          "0 = rechts; 1 = links",
          "0 = rechts auf links; 0.5 = keine; 1 = links auf rechts",
          "0 = rechts; 0.5 = keine; 1 = links",
          "in m")
dumTab<-data.frame(var,dummy1,dummy2)
knitr::kable(dumTab, position = "!b",
             caption = "Parameterbeschreibung zu Aufgabe 2 (Blatt 2)",
             col.names = c("Variable","Beschreibung","Codierung / Einheit"))
# All credits to https://www.r-bloggers.com/more-on-exploring-correlations-in-r/
cor.prob <- function (X, dfr = nrow(X) - 2) {
  R <- cor(X, use="pairwise.complete.obs",method="spearman")
  above <- row(R) < col(R)
  r2 <- R[above]^2
  Fstat <- r2 * dfr/(1 - r2)
  R[above] <- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] <- NA
  R
}
```

### Lösung a) 

Es soll sowohl für alle Parameter eine Deskription durchgeführt werden. Dies erfolgt getrennt für binäre/kategoriale und kontinuierliche Variablen.

```{r B2A2_2,eval=T}
setDT(myDat)

# binäre oder kategoriale Parameter
myDat[,.N,by=sex]
myDat[,.N,by=WHnd]
myDat[,.N,by=Fold]
myDat[,.N,by=Clap]

# kontinuierliche Parameter
tab1<-myDat[, sapply(.SD, summary),.SDcols=c("WrHnd","NWHnd","height")]
tab2<-myDat[, sapply(.SD, sd,na.rm = T),.SDcols=c("WrHnd","NWHnd","height")]
tab3<-myDat[, sapply(.SD, var,na.rm = T),.SDcols=c("WrHnd","NWHnd","height")]
tab<-rbind(tab1,tab2,tab3)
rownames(tab)[c(7,8)]<-c("SD","Var")
knitr::kable(tab, position = "!b",digits =3,
             caption = "Deskriptive Statistiken für die kontinuierlichen 
             Größen (Blatt 2, Aufgabe 2)",
             col.names = c("Länge Schreibhand",
                           "Länge der Nichtschreibhand",
                           "Größe"))
```

**Kommentar zu den NA's:** Man kann so filtern, dass nur noch vollständige Samples übrig bleiben. Allerdings muss man dann etwa 13% filtern. In der Praxis hat man fast immer unvollständige Daten und meistens reicht für die Deskription paarweise vollständige Daten aus. Deswegen wird hier nicht gefiltert, sondern immer erst beim jeweiligen Test. Falls man doch filtern möchte, sollte man einen Vektor erstellen, der in einem Schritt alle Samples mit NA's zusammenfasst:

```{r B2A2_3}
filt<-!is.na(myDat$sex) & !is.na(myDat$WrHnd) & !is.na(myDat$NWHnd) & !is.na(myDat$WHnd) & 
  !is.na(myDat$Fold) & !is.na(myDat$Clap) & !is.na(myDat$height) 
table(filt)
```

### Lösung b) 

Es soll die Differenz von Schreibhand zu Nicht-Schreibhand berechnet werden, und anschließend ein Ausreißer gefiltert werden. Dazu kann man entweder den Plot nutzen und den "offensichtlichen" Ausreißer filtern, oder man legt eine Grenze fest, zum Beispiel eine Abweichung von mehr als $4*SD$ vom Mittelwert.

```{r B2A2_4,eval=T}
myDat[,dif:=WrHnd-NWHnd]
filt<-myDat$dif>mean(myDat$dif,na.rm = T) + 4*sd(myDat$dif,na.rm = T) 
plot(myDat$id, myDat$dif, main="Handlängendifferenz", 
     xlab = "ID", ylab="Differenz",pch=16,cex.main=1.5,cex.lab=1.5)
points(myDat$id[filt],myDat$dif[filt],col="red",pch=16)
abline(h=mean(myDat$dif,na.rm = T) + 4*sd(myDat$dif,na.rm = T),lwd=2,col="red",lty=2)
myDat2<-myDat[!filt,]
attach(myDat2)
```

Das Samples 3 muss gefiltert werden, da die Differenz deutlich größer ist als bei allen anderen. 

### Lösung c) 

```{r B2A2_5,eval=T}
par(mfrow=c(2,2))
hist(WrHnd,breaks = 10,xlim = c(12,24),main="Histogram der Schreibhand",
     xlab="Schreibhandlänge")
boxplot(WrHnd ~ sex, main="Boxplot - Schreibhand")
hist(NWHnd,breaks = 10,xlim = c(12,24),main="Histogram der anderen Hand",
     xlab="Nicht-Schreibhandlänge")
boxplot(NWHnd ~ sex, main="Boxplot - andere Hand")
par(mfrow=c(1,1))
t.test(WrHnd ~ sex) 
t.test(NWHnd ~ sex) 
```

Ein Blick auf die Histogramme zeigt, dass die Daten von Schreib- und Nicht-Schreibhand gut genug normal-verteilt sind. Daher kann hier ein T-Test durchgeführt werden. Es gibt signifikante Unterschiede sowohl in der Schreibhand als auch in der anderen Hand.

### Lösung d)

Um die Differenz zu testen, kann man entweder einen gepaarten Test für Schreib- und Nicht-Schreibhand durchführen oder direkt die Differenz testen. Es gibt nur bei Frauen signifikante Unterschiede zwischen Schreib- und Nicht-Schreibhand, bei Männern nicht. 

```{r B2A2_6}
female<-sex==2
par(mfrow=c(2,2))
hist(dif[female],breaks = 10,main="Histogram der Differenz (Frauen)",
     xlab="Schreibhandlänge")
boxplot(WrHnd[female], NWHnd[female], main="Boxplot - Frauen")
hist(dif[!female],breaks = 10,main="Histogram der  Differenz (Männer)",
     xlab="Nicht-Schreibhandlänge")
boxplot(WrHnd[!female],NWHnd[!female], main="Boxplot - Männer")
par(mfrow=c(1,1))
t.test(WrHnd[female],NWHnd[female],paired = T)
t.test(WrHnd[!female],NWHnd[!female],paired = T)

```

### Lösung e)

Zuerst müssen alle unentschiedenen Fälle und NA's gefiltert werden. Übrig bleiben dann drei Vier-Felder-Tafeln, die man mit einem einfachen Fisher-Test vergleichen kann. Es gibt nur zwischen Klatschen und Händigkeit eine signifikante Beziehung. 

```{r B2A2_7}
# 1) Fold-Clap; 2) Fold-WHnd; 3) Clap-WHnd
filt1<-!is.na(Fold) & !is.na(Clap) & Fold!=0.5 & Clap!=0.5
table(Fold[filt1],Clap[filt1])
fisher.test(Fold[filt1],Clap[filt1]) 

filt2<-!is.na(Fold) & !is.na(WHnd) & Fold!=0.5 
table(Fold[filt2],WHnd[filt2])
fisher.test(Fold[filt2],WHnd[filt2]) 

filt3<-!is.na(Clap) & !is.na(WHnd) & Clap!=0.5 
table(Clap[filt3],WHnd[filt3])
fisher.test(Clap[filt3],WHnd[filt3]) 
```

### Lösung f) 

Der Funktions-Aufruf *cor.prob* aus Aufgabe 1 ignoriert automatisch alle NA's. Allerdings gibt man der Funktion die Anzahl der Freiheitsgrade mit, und falls es NAs gibt wird die damit verbundene Freiheitsgrad-Reduktion nicht berücksichtigt. Ich filter daher alle Zeilen mit NA. 

```{r B2A2_8}
filt<-!is.na(myDat2$sex) & !is.na(myDat2$WrHnd) & !is.na(myDat2$NWHnd) & !is.na(myDat2$WHnd) & 
  !is.na(myDat2$Fold) & !is.na(myDat2$Clap) & !is.na(myDat2$height) 
table(filt)
cor.prob(myDat2[female & filt,c(8,3,4,9)])
cor.prob(myDat2[!female & filt,c(8,3,4,9)])
```

Bei Frauen gibt es eine signifkante Korrelation zwischen der Länge der Nicht-Schreibhand und der Differenz der Handlängen, bei Männern nicht. 

### Lösung g)

```{r B2A2_9}
summary(lm(dif ~ NWHnd,subset=female))

```

Durch die Länge der Nicht-Schreibhand wird 14% der Varianz der Differenz erklärt. 

\newpage

# Blatt 3: Regressionsmodelle

Termin: 26.11.2019

Abgabe: Lineare Regression (Aufgabe 1)

## Nachtrag: Lineare Regression (Größe-Gewicht)

In der letzen Theorie-Übung konnte eine Hausaufgabe nicht besprochen werden, da es Probleme beim Hochladen auf Moodle gab. Diese wird daher hier wiederholt / ergänzt. 

Von 5 Personen sind Größe und Gewicht bekannt: 

```{r B3N1, eval=T,echo=F}
rm(list = setdiff(ls(), "plink_call"))
var<-c(1,2,3,4,5)
dummy1<-c(180,175,160,170,190)
dummy2<-c(80,80,58,60,85)
dumTab<-data.frame(var,dummy1,dummy2)
knitr::kable(dumTab, position = "!b",
             caption = "Tabelle zum Nachtrag (Blatt 3)",
             col.names = c("ID","Größe (cm)","Gewicht (kg) "))
```

Die Gleichung für die Residuenquadratsumme **RSS** ist:

$RSS(\beta_0,\beta_1) = \sum_{i=1}^{n}{[(\beta_0 + \beta_1*x_i)-y_i]^2}$

a) Bestimmen Sie den Schätzer $\hat{\beta_0}$ und $\hat{\beta_1}$, indem Sie RSS minimieren! (Hinweis: Partielle Ableitungen bzgl. $\beta_0$ und $\beta_1$, Nullsetzen und geeignet umformen.)

b) Schätzen Sie die Koeffizienten $\beta_0$ und $\beta_1$ für den Datensatz in Table 8! 

c) Welches Gewicht können Sie für eine 176 cm große Person vorhersagen? 

### Lösung a)
Die Summe läuft immer von $i=1$ bis $n$, daher wird nur $\sum$ verwendet. 

Partielle Ableitungen bzgl. $\beta_0$:
\begin{align}
\frac{\partial RSS(\beta_0,\beta_1)}{\partial \beta_0} &= 2 \sum (\beta_0 + \beta_1 x_i - y_i) \notag \\
&= 2n\beta_0 + 2\beta_1  \sum x_i - 2 \sum y_i \stackrel{!}{=} 0 \notag \\
\Longrightarrow \beta_0 &= \frac{\sum y_i - \beta_1  \sum x_i}{n} = \bar{y} - \beta_1 \bar{x} \notag 
\end{align}

Partielle Ableitungen bzgl. $\beta_1$:

Der Faktor 2 kann gekürzt werden, da man gleich 0 setzt. Anschließend wird $\beta_0$ eingesetzt. 
\begin{align}
\frac{\partial RSS(\beta_0,\beta_1)}{\partial \beta_1} &= 2 \sum (\beta_0 + \beta_1 x_i - y_i)x_i \notag \\
&=\beta_0 \sum x_i + \beta_1 \sum x_i^2 -\sum x_iy_i  \notag \\
&=\frac{1}{n} (\sum  y_i -\beta_1 \sum  x_i)\sum  x_i + \beta_1 \sum x_i^2 -\sum x_iy_i \notag \\
&= \frac{1}{n} \sum  y_i \sum  x_i - \frac{\beta_1}{n} (\sum  x_i)^2 + \beta_1 \sum x_i^2 -\sum x_iy_i \stackrel{!}{=} 0 \notag \\
\Longrightarrow \beta_1 ( \sum x_i^2 - \frac{1}{n} (\sum  x_i)^2) &= \sum  x_iy_i - \frac{1}{n} \sum  y_i \sum  x_i  \notag \\
\beta_1 &= \frac{\sum  x_iy_i - \frac{1}{n} \sum  x_i \sum  y_i}{\sum x_i^2 - \frac{1}{n} (\sum  x_i)^2} \notag \\
&= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum  (x_i - \bar{x})^2}\notag \\
\end{align}

### Lösung b) & c)

Die Aufgabe kann man mittels einsetzen in die obigen Formeln lösen, oder mittels linearer Regression. Die Beta-Schätzer werden dann zur Bestimmung des erwarteten Gewichts einer 176 cm großen Person verwendet. 

```{r B3N2, echo=T,eval=T}
# Vektoren erzeugen
groesse<-c(180,175,160,170,190)
gewicht<-c(80,80,58,60,85)

# Variante 1: per Hand
x_m<-mean(groesse)
y_m<-mean(gewicht)
var1<-sum((groesse-x_m)*(gewicht-y_m))
var2<-sum((groesse-x_m)^2)
b1<-var1/var2
b1
b0<-y_m - b1*x_m
b0

# Variante 2: mit lm
lm(gewicht ~ groesse)
summary(lm(gewicht ~ groesse))

# Gewicht fuer Person mit 176 cm schaetzen
b0+b1*176
```

\newpage

## Aufgabe 1: Lineare Regression 

Laden Sie den Datensatz **SNP.RData** in R ein:

```{r B3A1_1,eval=T,echo=F}
rm(list = setdiff(ls(), "plink_call"))
loaded1<-load("../data/SNP.RData")
loaded1
var<-colnames(myDat)
dummy1<-c("Durchlaufende ID-Nummer"	,"Geschlecht","Merkmal/Phänotyp","Genotyp")
dummy2<-c(NA,"1 = Mann; 2 = Frau","kontinuierlich","0 = AA; 1 = AB; 2 = BB")
dumTab<-data.frame(var,dummy1,dummy2)
knitr::kable(dumTab, position = "!b",
             caption = "Parameterbeschreibung zu Aufgabe 1 (Blatt 3)",
             col.names = c("Variable","Beschreibung","Codierung / Einheit"))
```

a) Untersuchen Sie die Effekte von *sex* und *SNP* auf *trait* mittels linearer Regression!

b) Untersuchen Sie die multiplen Effekte und die Interaktion der Einflussvariablen *sex* und *SNP*!

c) Welche von den vier Modellen ist besser geeignet, um *trait* zu beschreiben? Begründen Sie Ihre Entscheidung!

d) Welches genetische Modell wird hier verwendet?

e) Erstellen Sie je einen Wahrscheinlichkeits-Vektor pro Genotyp **AA**, **AB** und **BB** (1 = dieser Genotyp trifft zu, 0 = trifft nicht zu).

f) Erstellen Sie nun je einen Vektor pro genetischen Modell (additiv, dominant, rezessiv). (Hinweis: Überlegen Sie zuerst, welchen der Vektoren Sie bereits haben (Teilaufgabe d))

g) Untersuchen Sie die Effekte der verschiedenen Modelle auf *trait* mittels lineare Regression (univariat)! Welches Modell ist am besten geeignet? Wie könnten Sie Ihr Ergebnis testen?

h) Können wir annehmen, dass der SNP auf den Autosomen liegt, oder könnte es sich auch um einen X-chromosomalen SNP handeln? Begründen Sie Ihre Antwort. 

### Lösung a) - d)

```{r B3A1_2, echo=T,eval=T}
loaded1<-load("../data/SNP.RData")
loaded1

mod1<-lm(trait~sex,data=myDat)
mod2<-lm(trait~SNP,data=myDat)
mod3<-lm(trait~sex+SNP,data=myDat)
mod4<-lm(trait~sex*SNP,data=myDat)

summary(mod1)
summary(mod2)
summary(mod3)
summary(mod4)

dummy0<-c(summary(mod1)$adj.r.squared,summary(mod2)$adj.r.squared,
          summary(mod3)$adj.r.squared,summary(mod4)$adj.r.squared)
dummy1<-c(logLik(mod1),logLik(mod2),logLik(mod3),logLik(mod4))
dummy2<-AIC(mod1,mod2,mod3,mod4)
dumTab<-data.frame(dummy0,dummy1,dummy2)
rownames(dumTab)<-c("sex","SNP","sex+SNP","sex*SNP")
knitr::kable(dumTab, position = "!b",
             caption = "Modellvergleich univariat, multivariat, Interaktion (Blatt 3, Aufgabe 1)",
             col.names = c("adj. r^2","logLik","df","AIC"))

interaction.plot(myDat$sex,myDat$SNP,myDat$trait, fixed = T, 
                 col=c("red","green","blue"),lty=1,lwd=2,
                 xlab = "Geschlecht (1=Männer, 2=Frauen)",
                 trace.label = "SNP",ylab="Mittelwert Trait",
                 main="Interaktionsplot")
```

Modell 4 (Interaktion) hat sowohl größtes $r^2$, größten log-Likelihood und kleinsten AIC. Damit wäre das Modell das beste. Allerdings ist der Zugewinn an erklärter Varianz sehr gering, weshalb man abschätzen sollte, ob man wirklich die Interaktion mit in der Modell nimmt oder nur mit *sex* rechnet. 

Zur Interaktion: Diese kann man sehr gut in einem Interaktion-Plot erkennen. Wenn die Geraden kreuzen ist das immer ein Hinweis auf Interaktion. 

Hier wird ein additiver Effekt des *B*-Allels angenommen. Das erkennt man an der Codierung: kein *B* entspricht 0, ein *B* entspricht 1, und zwei *B* Allele entspricht 2. Der Effekt pro Allel summiert sich also. 

### Lösung e) - g)
Um alle genetischen Modelle zu bestimmen, werden zunächst die Wahrscheinlichkeitsvektoren pro Genotyp erstellt und anschließend geeignet summiert:

```{r B3A1_3, echo=T,eval=T}
wskAA<-c()
wskAA[myDat$SNP==0]<-1
wskAA[myDat$SNP!=0]<-0
wskAB<-c()
wskAB[myDat$SNP==1]<-1
wskAB[myDat$SNP!=1]<-0
wskBB<-c()
wskBB[myDat$SNP==2]<-1
wskBB[myDat$SNP!=2]<-0
add<-wskAB + 2*wskBB
table(add==myDat$SNP)

dom<-wskAB + wskBB
rez<-wskBB
```
Jetzt kann man mit *add*, *dom* und *rez* wieder lineare Modelle rechnen (hier wird nur auf die univariaten Modelle geschaut, die mit *sex* als Kovariable können analog berechnet werden):

```{r B3A1_4}
mod_add<-lm(myDat$trait ~ add)
mod_dom<-lm(myDat$trait ~ dom)
mod_rez<-lm(myDat$trait ~ rez)
summary(mod_add)
summary(mod_dom)
summary(mod_rez)
```

Das additive Modell hat sowohl die größte LogLikelihood als auch das kleinste AIC. Um zu testen, ob das wirklich das beste Modell ist, rechnen wir nun ein multivariates Modell mit den Wahrscheinlichkeitsvektoren von *AB* und *BB*:

```{r B3A1_5}
mod<-lm(myDat$trait ~ wskAB + wskBB)
summary(mod)

betas<-c(summary(mod_add)$coefficients[2,1],summary(mod_dom)$coefficients[2,1],
          summary(mod_rez)$coefficients[2,1],summary(mod)$coefficients[2,1],summary(mod)$coefficients[3,1])
dummy0<-c(summary(mod_add)$adj.r.squared,summary(mod_dom)$adj.r.squared,
          summary(mod_rez)$adj.r.squared,summary(mod)$adj.r.squared,summary(mod)$adj.r.squared)
dummy1<-c(logLik(mod_add),logLik(mod_dom),logLik(mod_rez),logLik(mod),logLik(mod))
dummy2<-AIC(mod_add,mod_dom,mod_rez,mod)
dummy2[5,]<-dummy2[4,]
dumTab<-data.frame(betas,dummy0,dummy1,dummy2)
rownames(dumTab)<-c("additiv","dominant","rezessiv","Effekt AB","Effekt BB")
knitr::kable(dumTab, position = "!b",
             caption = "Modellvergleich additiv, dominant, rezessiv (Blatt 3, Aufgabe 1)",
             col.names = c("beta","adj. r^2","logLik","df","AIC"))
```

Beide $\beta$-Schätzer sind signifikant von der 0 unterschiedlich. Es kann sich also nicht um das rezessive Modell handeln. Die beiden $\beta$ sind auch nicht gleich groß; das dominate Modell kann also auch verworfen werden. Das Verhältnis der $\beta$s ist zwar nicht exakt 1 zu 2, aber nah genug. Daher ist das additive Modell das passenste.   

### Lösung h) 
Wenn es sich um einen X-chromosomalen SNP handeln würde, sollte kein Mann den Genotyp *AB* haben, da diese für X haploid sind (nur *A* oder *B* möglich). Genotypiserungsfehler sind zwar nie auszuschließen, aber das ist ein sehr hoher Anteil, der gegen Chromosom X spricht. 

Es könnte sich um einen SNP in einer der pseudoautosomale Regionen (PAR) handeln, dann wären die Männer auch wieder diploid. 

Generell müsste dann das genetische Modell nochmal überdacht werden, je nachdem ob man X-Inaktivierung annimmt oder nicht. 

\newpage

## Aufgabe 2: Logistische / Proportional Odds Regression 

a) Berechnen Sie den **Median** von *trait* und nutzen Sie diesen als Cut-off, um *trait* in einen binären Phänotyp *trait2* zu zerlegen. 

b) Untersuchen Sie die univariaten und multivariaten Effekte von *sex* und *SNP* auf *trait2* mittels logistischer Regression! (Hinweis: Funktion *glm()* mit *family="binomial"*)

c) Bestimmen Sie die **Quartile** von *trait*! Zerlegen Sie *trait* nun in einen 4-stufigen Phänotypen *trait3*, in dem Sie die Quartile als Kategorien nutzen. 

d) Untersuchen Sie die univariaten und multivariaten Effekte von *sex* und *SNP* auf *trait3* mittels proportional odds regression! (Hinweis: Funktion *polr()* aus dem Paket **MASS** mit *Hess=T*)

e) Vergleichen Sie Ihre Ergebnisse von b) und d) mit den Ergebnissen von Aufgabe 1

### Lösung

In dieser Aufgabe soll der kontinuierlicher Phänotyp aus Aufgabe 1 in einen binären bzw. kategorialen zerlegt werden. Ein praktisches Beispiel, wo so etwas genutzt wird, ist der BMI. Er wird als kontinuierliches Merkmal bestimmt, aber je nach Analyse will man nur wissen, ob es Unterschiede zwischen Gruppen (Untergewicht (<20), Normalgewicht (20-25), Übergewicht (25-30) oder Adipositas (>30)) gibt, oder auch nur zwischen Übergewicht ja/nein. Die Ergebnisse ähneln sich meistens bezüglich der Signifikanz, allerdings sagen die Schätzer etwas anderes aus.

```{r B3A2, echo=T,eval=T}
trait_med<-median(myDat$trait)
trait_med
myDat$trait2<-0
myDat$trait2[myDat$trait>trait_med]<-1
table(myDat$trait2)

modB1<-glm(myDat$trait2~myDat$sex,family = "binomial")
modB2<-glm(myDat$trait2~myDat$SNP,family = "binomial")
modB3<-glm(myDat$trait2~myDat$SNP+myDat$sex,family = "binomial")
modB4<-glm(myDat$trait2~myDat$SNP*myDat$sex,family = "binomial")

summary(modB1)
summary(modB2)
summary(modB3)
summary(modB4)

quantile(myDat$trait)
#       0%      25%      50%      75%     100% 
# 33.87780 54.33411 60.33134 66.43621 87.46128
myDat$trait3<-0
myDat$trait3[myDat$trait<quantile(myDat$trait)[2]]<-1
myDat$trait3[myDat$trait>quantile(myDat$trait)[2] & myDat$trait<trait_med]<-2
myDat$trait3[myDat$trait>trait_med & myDat$trait<quantile(myDat$trait)[4]]<-3
myDat$trait3[myDat$trait>quantile(myDat$trait)[4]]<-4
table(myDat$trait3)

modD1<-polr(as.factor(myDat$trait3)~myDat$sex,Hess = T)
modD2<-polr(as.factor(myDat$trait3)~myDat$SNP,Hess = T)
modD3<-polr(as.factor(myDat$trait3)~myDat$SNP+myDat$sex,Hess = T)
modD4<-polr(as.factor(myDat$trait3)~myDat$SNP*myDat$sex,Hess = T)

summary(modD1)
summary(modD2)
summary(modD3)
summary(modD4)

```
Egal mit welchem Phänotypen man rechnet, dass Interaktionsmodell bleibt das beste. 

\newpage

## Aufgabe 3: Nichtlineare Regression 

```{r B3A3_1,eval=T,echo=F,warning=F}
rm(list = setdiff(ls(), "plink_call"))
loaded1<-load("../data/MichMenten.RData")
var<-colnames(myDat)
dummy1<-c("Substratkonzentration"	,
          "Umsatzgeschwindigkeit, gemessen in Erwachsenen",
          "Umsatzgeschwindigkeit, gemessen in Embryonen")
dummy2<-c("in 10^-5 mol", "in mikromol/(mg Enzym)*min","in mikromol/(mg Enzym)*min")
dumTab<-data.frame(var,dummy1,dummy2)
knitr::kable(dumTab, position = "!b", escape = T,
             caption = "Parameterbeschreibung zu Aufgabe 3 (Blatt 3)",
             col.names = c("Variable","Beschreibung","Codierung / Einheit"))
```

Die Michaelis-Menten-Gleichung ist gegeben als: $v=\frac{V_{max}*c(S)}{c(S) + K_m}$

a) Was beschreibt die Michaelis-Menten-Gleichung? Wofür stehen $V_{max}$ und $K_m$? 

b) Betrachten Sie den Datensatz **MichMenten.RData**. Bestimmen Sie $V_{max}$ und $K_m$ für Erwachsene und Embryonen getrennt, indem Sie die Funktion *nls()* und folgende Startwerte nutzen: $V_{max}=max(v)$ und $K_m=\frac12 max(v)$

c) Fassen Sie die Ergebnisse in einem Plot zusammen und interpretieren Sie diesen!

### Lösung

Die Michaelis-Menten-Gleichung beschreibt eine Sättigungsfunktion: Umsatzgeschwindigkeit einer enzymatischen Reaktion in Abhängigkeit der Substratkonzentration. 

$V_{max}$: maximale Geschwindigkeit

$K_{m}$: Michaelis-Menten-Konstante

Abhängig von Reaktion und Milieu (z.B. pH-Wert)

```{r B3A3_2, echo=T,eval=T}
loaded2<-load("../data/MichMenten.RData")
loaded2
colnames(myDat)
dim(myDat)
attach(myDat)
# Startwerte: v_max=max(v), K_m=0.5*max(v)
vmaxA<-max(myDat$vA)
vmaxE<-max(myDat$vE)
kmA<-vmaxA/2
kmE<-vmaxE/2
# Nichtlineare Regression
modA<-nls(vA ~ vmax*cS/(cS+km), start = list(km=kmA,vmax=vmaxA))
modE<-nls(vE ~ vmax*cS/(cS+km), start = list(km=kmE,vmax=vmaxE))
summary(modA)
summary(modE)

FuncA<-function(z) {summary(modA)$parameters[2,1]*z/(z+summary(modA)$parameters[1,1])}
FuncE<-function(z) {summary(modE)$parameters[2,1]*z/(z+summary(modE)$parameters[1,1])}

z<-seq(0:300)
par(mar=c(5,6,5,5))
plot(cS,vA,
     main = "Michaelis-Menten Kinetik",cex.main=1.5,
     xlim=c(1,300),
     xlab="Konzentration [10^-5 mol]",cex.lab=1.5,
     ylim=c(1,20),cex.axis=1.5,
     ylab=expression(paste("Umsatzgeschwindigkeit ","[", mu, "mol/(mg*min)]")),
     col="black",pch=16,cex=1.5)
lines(FuncA(z),lwd=3,lty=3)
points(cS,vE,col="red",pch=16,cex=1.5)
lines(FuncE(z),col="red",lwd=3,lty=3)

abline(h=summary(modA)$parameters[2,1],lwd=2,lty=2,col="black")
abline(v=summary(modA)$parameters[1,1],lwd=2,lty=2,col="black")
abline(h=0.5*summary(modA)$parameters[2,1],lwd=2,lty=2,col="black")

abline(h=summary(modE)$parameters[2,1],lwd=2,lty=2,col="red")
abline(v=summary(modE)$parameters[1,1],lwd=2,lty=2,col="red")
abline(h=0.5*summary(modE)$parameters[2,1],lwd=2,lty=2,col="red")

```

Die maximale Geschwindigkeit ist ähnlich, aber $K_M$ unterscheidet sich sehr zwischen Erwachsenen und Embryos. 

\newpage

# Blatt 4: Visualisierung stat. Konzepte

Termin: 10.12.2019

Abgabe: XY-Plots (Aufgabe 2)

## Aufgabe 1: Verwandtschaft

Zur paarweisen Schätzung von Verwandtschaften haben Sie in der Vorlesung den Kinship-Schätzer kennen gelernt: 

$$ \hat k_{i,j} = \frac{1}{M} \sum_{m=1}^{M} \frac{(g_{m,i}-2p_{m,B})(g_{m,j}-2p_{m,B})}{4p_{m,B}p_{m,A}} $$

mit 

* $M$ als Anzahl der betrachteten biallelischen SNPs (Allel A und B)

* $p_{m,B}$ als Allelfrequenz des SNPs m bezüglich Allel B

* $g_{m,i}$ als Genotyp des SNPs m von Person i bezüglich Allel B

a) Laden Sie den Datensatz **verwandtschaft.RData** in R ein! Stellen Sie die Verteilungsdichte der Allelfrequenzen (Variable *allelfreq*) graphisch dar! Welcher Wahrscheinlichkeitsverteilung unterliegen die Allelfrequenzen vermutlich?

b) Bestimmen Sie alle paarweisen Verwandtschaften mit dem Kinship-Schätzer!

c) Warum gilt: $$ \hat k_{i,j} \approx 0.5$$

d) Wie viele paarweise Verwandtschaften (von Grad 1,2, … , unverwandt) beobachten Sie?

e) Welche Familienstruktur könnte die beobachteten Verwandtschaftsbeziehungen erklären?

### Lösung a)

```{r B4A1_1, eval=T, echo=T}
loaded<-load("../data/verwandtschaft.RData")
loaded
dim(genotypes)
class(genotypes)
length(allelfreq)
hist(allelfreq)
```
Im Datensatz sind zwei Parameter, einmal *genotypes*, eine Genotyp-Matrix von 10 Personen mit 30,000 SNPs, und *allelfreq*, ein Vektor der Allelfrequenzen pro SNP aus *genotypes*. 

Im Histogramm kann man erkennen, dass die Allelfrequenzen in etwa gleichverteilt sind. Das heißt, dass die *rare* SNPs (Allelfrequenz<5%) gleich-häufig sind wie die *common* SNPs. 

### Lösung b)

Es soll die paarweise Verwandtschaft mittels Kinship-Schätzer berechnet werden. Das geht sehr schnell als Matrixoperation, aber auch schrittweise mittels zweier Schleifen. 

Bei der Matrixoperation erstellt man sich erst eine Hilfsmatrix *g*, in der die Adjustierungen aller *M* SNPs durchgeführt wird, *g* hat also die gleiche Dimension wie *genotypes*. Nach der Adjustierung ist der Kinship-Schätzer lediglich das Matrixprodukt von $g^T * g$. 

```{r B4A1_2}
n=ncol(genotypes) 
m=nrow(genotypes) 
g=(genotypes-matrix(2*allelfreq,m,n))/sqrt(m*matrix(4*allelfreq*(1-allelfreq),m,n))
G=t(g)%*%(g)
```
Für die Schleifen-Variante erstellt man sich zunächst auch Hilfsvektoren für die Adjustierung, und bestimmt dann in jedem Schleifendurchgang für Individuum *i* den Kinship-Schätzer der *n-i* Personen. 

```{r B4A1_3}
p2<-2*allelfreq
p4<-4*allelfreq*(1-allelfreq)
genotypes1<-genotypes-p2

K<-matrix(NA,10,10)
for(i in 1:10){
  # i=1
  for(j in i:10){
    # j=2
    g_i<-genotypes1[,i]
    g_j<-genotypes1[,j]
    x<-sum(g_i*g_j/p4)
    k_ij<-x/30000
    K[i,j]<-k_ij
    K[j,i]<-k_ij
  }
    
}

knitr::kable(K,digits = 3, caption = "Verwandschaftsmatrix Kinship-Schätzer (Aufgabe 1, Blatt 4)")

table(round(G,4)==round(K,4))
```
Die sich ergebenden Matrizen *G* und *K* sind identisch. Für die paarweise Verwandtschaft brauchen wir nur die obere Dreiecksmatrix. 

### Lösung c)

Auf der Diagonalen selbst sollte immer 0.5 stehen, das ist für den Kinship-Schätzer Identität oder eineigige Zwillinge. 

### Lösung d)

$k_{i,j}$ | Interpretation
----------|-------------------------------------------------------
0.5 | Eineigige Zwillinge / Identität
0.25 | erstgradige Verwandtschaft (z.B. Eltern-Kind, Geschwister)
0.125| zweitgradige Verwandtschaft (z.B. Halbgeschwister, Großeltern-Enkel, Onkel/Tante-Nichte/Neffe)

Anzahl Verwandtschaften: 

* 18 unverwandte Paare

* 12 mal Großeltern-Enkel, Onkel/Tante-Nichte/Neffe oder Halbgeschwister

* 15 mal Eltern-Kinder oder Geschwister

### Lösung e)

```{r B4A1_4}
table(round(diag(G),1))
table(round(G[lower.tri(G)],2))

V=G
V[G<.3]=1 
V[G<.2]=2  
V[G<.1]=0                                                            
V[col(G)<=row(G)]=NA                                                          
colnames(V)<- c("S1","S2","S3","S4","S5","S6","S7","S8","S9","S10")
knitr::kable(V, caption = "Verwandschaftsgrade (Aufgabe 1, Blatt 4)")
```
Interpretation 1: Ein Vater (1) von sechs Kinder (5 - 10), drei Mütter (2, 3, 4) von je zwei Kindern.

Interpretation 2: Eine Mutter (1) von sechs Kinder (5 - 10), drei Väter (2, 3, 4) von je zwei Kindern.

![Graphische Darstellung der Verwandtschaftsbeziehungen](Blatt4_Verwandtschaft.jpg)
\newpage 

## Aufgabe 2: XY-Plot

Betrachten Sie den Datensatz **XYPlots.RData**.

a) Machen Sie sich mit der Struktur des Datensatzes vertraut! Wo stehen die ID? Wie viele X- bzw. Y-SNPs gibt es? Wie viele Männer, wie viele Frauen?

b) Für den XY-Plot brauchen wir die Gesamtintensitäten. Bilden Sie daher zuerst den Mittelwert der Intensität für Allel A und B und bilden Sie dann die jeweilige mittlere Intensität aller X-SNPs und aller Y-SNPs pro Sample! 

c) Zusätzlich wollen wir die Heterozygosität auf Chromosom X bestimmen. Bestimmen Sie dazu pro Sample die Genotyphäufigkeiten (AA, AB, BB) aller SNPs von Chromosom X und berechnen Sie den Anteil von AB an allen Genotypen!

d) Sie sollten nun ein Objekt mit den Variablen *ID*, *X-Intensität*, *Y-Intensität,* *X-Heterozygosität*, *sex_datenbank* und *sex_computed* pro Sample haben. Erzeugen Sie nun folgende drei Plots:
  
    i.	X-Intensität – Y-Intensität
  
    ii.	X-Intensität – X-Heterozygosität
  
    iii.	Y-Intensität – X-Heterozygosität
    
Markieren Sie alle auffälligen Ausreißer (z.B. widersprüchliche Geschlechtsangabe, …)

### Lösung a)

```{r B4A2_1}
rm(list = setdiff(ls(), "plink_call"))
loaded<-load("../data/XYPlots.RData")
loaded
dim(samples);colnames(samples)
rownames(samples)[c(1:10,291:300)]
dim(daten);colnames(daten)[c(1,200,201,300)]
rownames(daten)[1:3]
```
Es liegen Daten von 300 SNPs und 300 Samples vor. Zu den Samples (IDs 1-300) ist das Geschlecht mitangegeben, einmal aus der Datenbank (*sex_datenbank*) und einmal wie es beim Calling bestimmt wurde (*sex_computed*). Die SNPs haben die IDs mit Chromosomenangabe, d.h. die X-chromosomalen SNPs haben die IDs *X:1* - *X:200*, die Y-chromosomalen *Y:1* - *Y:100*. Pro SNP und Sample liegen die Intensitäten der Allele *A* und *B* und der wahrscheinlichste Genotyp vor (0=*AA*, 1=*AB*, 2=*BB*).

### Lösung b)

Zum einfacheren Bearbeiten werden die Genotypdaten von den Intensitäten getrennt. Anschließend kann pro SNP und Sample der Mittelwert der Allel-Intensitäten bestimmt werden. Abschließend kann die mittlere Intensität der X- und Y-SNPs pro Sample bestimmt werden. Zusätzlich kann man die Intensiäten am Ende noch normieren, was die Interpretation der Daten vereinfachen kann. 

```{r B4A2_2}
# Daten trennen
filt<-grepl("genotype",rownames(daten))
table(filt)
geno<-daten[filt,]
intent<-daten[!filt,]

# Mittelwert pro SNP und Sample
all<-seq(from=1,to=dim(intent)[1],by=2)
data.a<-intent[all,]             
data.b<-intent[all+1,]
dataInt<-(data.a+data.b)/2   

# mittlere Intensitäten pro Chromosom
dataIntX<-dataInt[,1:200]
dataIntY<-dataInt[,201:300]
IntX<-rowMeans(dataIntX)          
IntY<-rowMeans(dataIntY)          

# Normierung der Intensitäten nach dem 75%-Quantil
IntX2<-IntX/boxplot(IntX,plot=F)$stats[4]
IntY2<-IntY/boxplot(IntY,plot=F)$stats[4]
```

### Lösung c)

Die X-Heterozygosität ist ein weiterer Marker der Qualität: Männer haben nur ein X, daher sollten sie eine Heterozygosität von 0 haben. Für Frauen erwartet man 0.25 (Erwartungswert der Heterozygoten bei eine Beta-Verteilung mit $\alpha=\beta=0.5$). 

Herleitung des Erwartungswert mit beta-verteilter Allelfrequenz *x*: 

\begin{align}
E(X=AB) &= \int 2*x*(1-x) *\frac{x^{\alpha-1}*(1-x)^{\beta-1}}{B(\alpha,\beta)} dx \notag \\
&= \int \frac{2}{B(\alpha,\beta)} x^{\alpha} * (1-x)^{\beta} dx \notag \\
&= \frac{2}{B(\alpha,\beta)} * \int x^{\alpha} * (1-x)^{\beta} dx \notag \\
&= \frac{2*B(\alpha+1,\beta+1)}{B(\alpha,\beta)} \notag \\
&= 2*0.3926991/3.141593 = 0.25 \notag
\end{align}

```{r B4A2_3}
snpDataX<-geno[,1:200]
dim(snpDataX)
snpDataX<-t(snpDataX)
countGenos<-function(x) {return(as.numeric(table(factor(x,levels=-1:2))))}
nrGenos<-apply(snpDataX,2,countGenos)
nrGenos<-t(nrGenos)
colnames(nrGenos)<-c("miss","AA","AB","BB")
heteroRate<-apply(nrGenos,1,function(x) x[3]/sum(x[2:4]))
```

### Lösung d)

Jetzt kann man die Daten pro Sample zusammenfassen und die Plots erstellen. Es ist hilfreich, sich Labels oder eine Ordnung zu überlegen. In diesem Beispiel werden die Samples nach ihrer Zusammensetzung aus Datenbank-Geschlecht und berechneten erstellt. Damit wird dann im Plot das Label erzeugt. Zusaätzlich werden die Samples danach sortiert. Der Grund hierfür ist, dass **ggplot2** geordnet plottet, sodass gegebenenfalls einzelne Labels nicht mehr sichtbar sind (z.B. wenn die Sex-Mismatches zuerst geplottet werden und dann darüber die "normalen" Männer und Frauen kommen). 

```{r B4A2_4}
myDat<-data.frame(samples,IntX,IntY,IntX2,IntY2,heteroRate)
colnames(myDat)

table(myDat$sex_datenbank,myDat$sex_computed)

myDat$sexLabel[myDat$sex_datenbank=="male"&myDat$sex_computed=="male"]<-"1/male"
myDat$sexLabel[myDat$sex_datenbank=="male"&myDat$sex_computed=="unknown"]<-"1/unknown"
myDat$sexLabel[myDat$sex_datenbank=="male"&myDat$sex_computed=="female"]<-"1/female"
myDat$sexLabel[myDat$sex_datenbank=="female"&myDat$sex_computed=="male"]<-"2/male"
myDat$sexLabel[myDat$sex_datenbank=="female"&myDat$sex_computed=="unknown"]<-"2/unknown"
myDat$sexLabel[myDat$sex_datenbank=="female"&myDat$sex_computed=="female"]<-"2/female"

table(myDat$sexLabel)

myDat$category[myDat$sexLabel == "1/male"] <- 1
myDat$category[myDat$sexLabel == "2/female"] <- 2
myDat$category[myDat$sexLabel == "1/unknown"] <- 3
myDat$category[myDat$sexLabel == "2/unknown"] <- 4
myDat$category[myDat$sexLabel == "1/female"] <- 5
myDat$category[myDat$sexLabel == "2/male"] <- 6

ordering<-order(myDat$category)
myDat<-myDat[ordering,]

# jetzt plotten
myPlot1 <- ggplot() +
  geom_point(data=myDat,aes(x=IntX2,y=IntY2,color=sexLabel,shape=sexLabel),size=4) +
  xlab("X Intensität") + ylab("Y Intensität") + ggtitle("XY Plot mit 300 Samples") +
  scale_colour_manual(name="submitted/computed",
                      values=c("red","blue","orange","darkseagreen","cyan","magenta")) +
  scale_shape_manual(name="submitted/computed",values=c(17,17,17,19,19,19)) +
  theme(legend.justification=c(1,1),legend.text=element_text(size=10),
        legend.title=element_text(size=10)) +
  theme(axis.text=element_text(size=10), axis.title=element_text(size=10),
        plot.title=element_text(size=15))
myPlot1

myPlot2 <- ggplot() +
  geom_point(data=myDat,aes(x=IntX2,y=heteroRate,color=sexLabel,shape=sexLabel),size=4) +
  xlab("X Intensität") + ylab("X Heterozygosität") + ggtitle("XX Plot mit 300 Samples") +
  scale_colour_manual(name="submitted/computed",
                      values=c("red","blue","orange","darkseagreen","cyan","magenta")) +
  scale_shape_manual(name="submitted/computed",values=c(17,17,17,19,19,19)) +
  theme(legend.justification=c(1,1),legend.text=element_text(size=10),
        legend.title=element_text(size=10)) +
  theme(axis.text=element_text(size=10), axis.title=element_text(size=10),
        plot.title=element_text(size=15))
myPlot2
```
Man kann folgende Ausreißer erkennen: 

* Frauen mit zu hoher oder zu niedriger X-Intensität (Mono-X oder Triple-X Frauen)

* Männer mit zu hoher Y-Intensität (Doppel-Y Männer)

* Männer mit zu hoher X-Intensität (Doppel-X Männer)

* Frauen mit zu hoher oder zu niedriger X-Heterozygosität 

* Samples mit Sex-Mismatches zwischen Datenbank und Berechnung

Diese kann man mit einem Filter auswählen und im Plot markieren. Die Sex-Mismatches müssen für alle Analysen gefiltert werden, da man hier nur von einem Auftragungsfehler ausgehen kann. Die anderen Samples sind für autosomale Analysen ok. Für Chromosom X oder Y Analysen sollten sie allerdings ebenfalls gefiltert werden. 

```{r B4A2_5}
# Auffaelligkeiten: 10 Samples
filter_1<-myDat$sexLabel=="2/female"&((myDat$IntX2>1.1)|(myDat$IntX2<0.9))
filter_2<-myDat$sexLabel=="1/male"&myDat$IntY2>1.1
filter_3<-myDat$sexLabel=="1/unknown"&myDat$IntX2>0.9
filter_4<-myDat$sex_datenbank=="female"&((myDat$heteroRate>0.35)|(myDat$heteroRate<0.15))
filter_5<-myDat$sexLabel=="2/male"|myDat$sexLabel=="1/female"

circleObject<- myDat[filter_1 | filter_2 | filter_3 | filter_4 | filter_5,]

# jetzt plotten
myPlot1 <- ggplot() +
  geom_point(data=myDat,aes(x=IntX2,y=IntY2,color=sexLabel,shape=sexLabel),size=4) +
  xlab("X Intensität") + ylab("Y Intensität") + ggtitle("XY Plot mit 300 Samples") +
  scale_colour_manual(name="submitted/computed",
                      values=c("red","blue","orange","darkseagreen","cyan","magenta")) +
  scale_shape_manual(name="submitted/computed",values=c(17,17,17,19,19,19)) +
  theme(legend.justification=c(1,1),legend.text=element_text(size=10),
        legend.title=element_text(size=10)) +
  theme(axis.text=element_text(size=10), axis.title=element_text(size=10),
        plot.title=element_text(size=15)) +  
  geom_point(data=circleObject,aes(x=IntX2,y=IntY2),color="black",size=10,shape=21)
myPlot1

myPlot2 <- ggplot() +
  geom_point(data=myDat,aes(x=IntX2,y=heteroRate,color=sexLabel,shape=sexLabel),size=4) +
  xlab("X Intensität") + ylab("X Heterozygosität") + ggtitle("XX Plot mit 300 Samples") +
  scale_colour_manual(name="submitted/computed",
                      values=c("red","blue","orange","darkseagreen","cyan","magenta")) +
  scale_shape_manual(name="submitted/computed",values=c(17,17,17,19,19,19)) +
  theme(legend.justification=c(1,1),legend.text=element_text(size=10),
        legend.title=element_text(size=10)) +
  theme(axis.text=element_text(size=10), axis.title=element_text(size=10),
        plot.title=element_text(size=15)) +  
  geom_point(data=circleObject,aes(x=IntX2,y=heteroRate),color="black",size=10,shape=21)
myPlot2

myPlot3 <- ggplot() +
  geom_point(data=myDat,aes(x=IntY2,y=heteroRate,color=sexLabel,shape=sexLabel),size=4) +
  xlab("Y Intensität") + ylab("X Heterozygosität") + ggtitle("YX Plot mit 300 Samples") +
  scale_colour_manual(name="submitted/computed",
                      values=c("red","blue","orange","darkseagreen","cyan","magenta")) +
  scale_shape_manual(name="submitted/computed",values=c(17,17,17,19,19,19)) +
  theme(legend.justification=c(1,1),legend.text=element_text(size=10),
        legend.title=element_text(size=10)) +
  theme(axis.text=element_text(size=10), axis.title=element_text(size=10),
        plot.title=element_text(size=15)) +  
  geom_point(data=circleObject,aes(x=IntY2,y=heteroRate),color="black",size=10,shape=21)
myPlot3

```
\newpage 

## Zusatz: PCA

In dieser Aufgabe sollen die Eigenvektoren von genetischen Daten erzeugt werden. Dafür wird ein Teil der Daten von 1000 Genomes (1KG, Phase 1, release 3) verwendet. Zusätzlich zu R wird hier PLINK verwendet, da dieses Programm effizienter große Datenmengen verarbeiten kann [PLINK](https://www.cog-genomics.org/plink/1.9/). 

a) **Datenvorbereitung - SNPs filtern**: Überprüfen Sie in R, ob alle SNPs von **mySNPs.txt** in 1KG sind. Hierfür sollte man am besten das **1KG_PCA.bim** File verwenden (tab-delimited). Recherchieren Sie, was in den einzelnen Spalten des .bim Files steht. Nutzen sie zum Einlesen der Befehl *fread()* aus dem Paket „*data.table*“. Filtern Sie nach den SNPs in der Schnittmenge und erstellen Sie ein gefiltertes Text-File mySNPs_filtered.txt!

b) **Datenvorbereitung - Samples filtern**: Erstellen Sie eine Sample Liste mit Individuen aus Asien, Afrika und Europa! Nutzen Sie hierfür das **1KG_PCA.fam** File (space-delimited). Wir wollen eine möglichst große Menge an Samples, aber jeder Herkunft sollte gleich oft vorhanden sein! Ziehen Sie zufällig aus der jeweiligen Teilmenge und speichern Sie ihre Liste als **mySamples.txt** ab!

c) **Datenvorbereitung - SNPs prunen**: Jetzt prunen Sie die SNPs mit PLINK, d.h. Sie prüfen, welche SNPs in hohem LD miteinander sind. Folgende Parameter sollten Sie setzen:
  i.	Input:--bfile 1KG_PCA
  ii.	SNPs einschränken: --extract mySNPs_filtered.txt
  iii.	Samples einschränken: --keep mySamples.txt
  vi.	LD-Pruning-Parameter festlegen: --indep-pairphase 50 5 0.2 
  v.	Output: --out pruned_filter
  
d) **Datenvorbereitung - Datensatz erstellen**: Erstellen Sie jetzt mit PLINK ein neues .bed-File, dass nur noch die geprunten SNPs und die gewünschten Samples (aus 2) enthält (--bfile, --extract, --keep, und --make-bed). 

e) **PCA berechnen**: Berechnen Sie jetzt mit den neuen Files die PCA (--bfile, --pca, --out)

f) **PCA auswerten**: Laden Sie beide Outputs der PCA in R ein! Wie sind die Daten aufgebaut? Erstellen Sie einen Plot der ersten beiden Vektoren mit Ethnien-Färbung! Was kann man daraus schließen? Berechnen Sie den Anteil der erklärten Varianz
  i.	durch den ersten Eigenvektor und
  ii.	durch die ersten beiden Eigenvektoren!

g) Was würden Sie erwarten, wenn alle 4 Ethnien in die Analyse eingeflossen wären? Wo würden Sie die Amerikaner einordnen? Rechnen Sie das nach!

```{r B4Z0, echo=F,eval=T}
rm(list = setdiff(ls(), "plink_call"))
```

### Lösung a)

|Spalte|Information|
|------|-----------|
|V1|Chromosom|
|V2|SNP-ID|
|V3|Position in Morgan oder Centimorgan -> auf 0 gesetzt |
|V4|Position in Basenpaaren|
|V5|Allel 1 (gewöhnlich minor)|
|V6|Allel 2 (gewöhnlich major)|

```{r B4Z1, eval=T,echo=T,cache=T}
myTab<-read.table("../data/mySnps.txt")
dim(myTab)
rslist<-fread("../data/1KG_PCA.bim",sep="\t",stringsAsFactors=F)
dim(rslist)
head(rslist)

table(is.element(myTab$V1,rslist$V2))
filt<-is.element(myTab$V1,rslist$V2)
dummy<-as.character(myTab$V1[filt])
write.table(dummy,file="results_PCA/mySnps_filtered.txt", quote=F,row.names=F,col.names=F)
```

In **mySNPs** sind insgesamt 224458 SNPs enthalten, von denen 206233 auch in dem 1000Genomes Datensatz sind. 

### Lösung b)

|Spalte|Information|
|------|-----------|
|V1|Family ID|
|V2|Within-family ID mit Ethnie|
|V3|Within-family ID des Vaters |
|V4|Within-family ID der Mutter|
|V5|Sex Code (1=male, 2=female, 0=unknown)|
|V6|Phenptyp (1=control, 2=case, -9,0,non-numeric=missing data)|

```{r B4Z2, eval=T,echo=T, cache=T}
fam.data<-read.table("../data/1KG_PCA.fam",stringsAsFactors=F,sep=" ")
dim(fam.data)
head(fam.data)
ethno<-substr(fam.data$V2,1,3)
table(ethno)

v.ethno<-c("AFR","ASN","EUR")
n.ethno<-min(table(ethno)[v.ethno])
samp.auswahl<-rep(F,length(ethno))
set.seed(2)
for(i in v.ethno){
  samp.auswahl[ethno==i] <-  1:sum(ethno==i) %in% sample(sum(ethno==i),n.ethno)
}
table(ethno[samp.auswahl])
fam.data.restr<-fam.data[samp.auswahl,]

write.table(fam.data.restr,file="results_PCA/mySamples.txt",
            quote=F,row.names=F,col.names=F)
```

In 1000Genomes sind insgesamt 1092 Samples enthalten. Die geringste Zahl ist bei Afrikanern (n=246), daher nimmt man hiervon alle und wählt zufällig 246 Europäer und Asiaten. Insgesamt bleiben 3*246 Samples für die PCA. 

### Lösung c)
Die drei Zahlen hinter dem --indep-pairphase Befehl bedeuten:

* Fenstergröße --> 50 benachbarten SNPs

* Anzahl an SNPs zum Verschieben des Fensters --> pro Step 5 SNPs weiter 

* LD Threshold zum Prunen --> SNPs mit $r^2>0.2$ werden gefiltert (basierend auf Maximum Likelihood phasing)

```{r B4Z3, eval=T,echo=T,cache=T}
datafn<-"--bfile ../data/1KG_PCA"
snplistfn<-"--extract results_PCA/mySnps_filtered.txt"
samplelistfn<-"--keep results_PCA/mySamples.txt"
ld<-"--indep-pairphase 50 5 0.2"
outfn<-"--out results_PCA/pruning_filter"
call1<-paste(plink_call,datafn,snplistfn,samplelistfn, ld,outfn, sep=" ")
system(call1)
```

Insgesamt bleiben 121878 geprunte SNPs für die PCA, 84355 SNPs in LD werden gefiltert. 

### Lösung d) & e)

```{r BB4Z4, eval=T,echo=T,cache=T}
snplistfn<-"--extract results_PCA/pruning_filter.prune.in"
outfn<-"--make-bed --out results_PCA/pruned_data"
call2<-paste(plink_call,datafn,snplistfn,samplelistfn,outfn, sep=" ")
system(call2)

datafn<-"--bfile results_PCA/pruned_data"
outfn<-"--pca --out results_PCA/pca_out"
call3<-paste(plink_call,datafn,outfn, sep=" ")
system(call3)
```

### Lösung f)

```{r BB4Z5, eval=T,echo=T,cache=T}
pca2values<-read.table("results_PCA/pca_out.eigenval")$V1
pca2vector<-read.table("results_PCA/pca_out.eigenvec",stringsAsFactors=F,sep=" ")

# Anteil der erklärten Varianz
(pca2values[1])/sum(pca2values)
(pca2values[1]+pca2values[2])/sum(pca2values)

# PCA Plot (EV1 vs EV2)
xmin<-min(pca2vector[,3]);xmax<-max(pca2vector[,3])
ymin<-min(pca2vector[,4]);ymax<-max(pca2vector[,4])

plot(0,0,col="white",xlim=c(xmin,xmax),ylim=c(ymin,ymax),
     main="PCA 1000Genomes (3*246 Samples, 121878 geprunte SNPs)",
     xlab="1. Hauptkomponente",ylab="2. Hauptkomponente")
lines(pca2vector[substr(fam.data.restr$V2,1,3)=="AFR",c(3,4)],
      col=alpha("black",0.1),type="p",pch=19,cex=1.9)
lines(pca2vector[substr(fam.data.restr$V2,1,3)=="ASN",c(3,4)],
      col=alpha("red",0.1),type="p",pch=19,cex=1.9)
lines(pca2vector[substr(fam.data.restr$V2,1,3)=="EUR",c(3,4)],
      col=alpha("blue",0.1),type="p",pch=19,cex=1.9)
legend("bottomleft",legend=v.ethno,col=c("black","red","blue"),pch=19)

```

Der erste Eigenwert erklärt 47.3% der genetischen Varianz in der Population, der zweite Eigenwert noch 30.2% (bezogen auf die ersten 20 Eigenwerte). Beide zusammen erklären etwa 77.5% der Varianz. 

Dies kann man im Plot der ersten Eigenvektoren gut erkennen: Es sind drei Cluster zu erkennen, die spezifisch für die Ethnien sind. Das heißt, die ersten Hauptkomponenten bilden die Ethnie ab und bieten sich zur Adjustierung auf die Populationsstruktur an (Stichwort Stratifikationsbias). 

### Lösung g) 

Um die Analyse für alle Samples zu wiederholen sollte man auch das Pruning wiederholen. Hier bleiben diesmal 118472 SNPs für die PCA übrig. 

Die erklärten Varianzen bleiben ähnlich (EW 1: 45.6%, EW 1 & 2: 77.3%).

Im Plot der ersten Eigenvektoren sind nun auch die Amerikaner eignefärbt. Aufgrund der Einwanderung sind diese in der Nähe des europäischen Clusters, aber durch die Durchmischung der Population über die letzten Jahrhunderte bilden die Amerikaner eher ein kleines Dreieck im großen Dreieck.  

```{r BB4Z6, eval=T,echo=T,cache=T}
datafn<-"--bfile ../data/1KG_PCA"
snplistfn<-"--extract results_PCA/mySnps_filtered.txt"
outfn<-"--out results_PCA/pruning_filter2"
call4<-paste(plink_call,datafn,snplistfn,ld,outfn, sep=" ")
system(call4)

snplistfn<-"--extract results_PCA/pruning_filter2.prune.in"
outfn<-"--make-bed --out results_PCA/pruned_data2"
call5<-paste(plink_call,datafn,snplistfn,outfn, sep=" ")
system(call5)

datafn<-"--bfile results_PCA/pruned_data2"
outfn<-"--pca --out results_PCA/pca_out2"
call6<-paste(plink_call,datafn,outfn, sep=" ")
system(call6)

pca2values<-read.table("results_PCA/pca_out2.eigenval")$V1
pca2vector<-read.table("results_PCA/pca_out2.eigenvec",stringsAsFactors=F,sep=" ")

(pca2values[1])/sum(pca2values)
(pca2values[1]+pca2values[2])/sum(pca2values)

xmin<-min(pca2vector[,3]);xmax<-max(pca2vector[,3])
ymin<-min(pca2vector[,4]);ymax<-max(pca2vector[,4])

plot(0,0,col="white",xlim=c(xmin,xmax),ylim=c(ymin,ymax),
     main="PCA 1000Genomes (1092 Samples, 118472 geprunte SNPs)",
     xlab="1. Hauptkomponente",ylab="2. Hauptkomponente")
lines(pca2vector[substr(fam.data$V2,1,3)=="AFR",c(3,4)],
      col=alpha("black",0.1),type="p",pch=19,cex=1.9)
lines(pca2vector[substr(fam.data$V2,1,3)=="ASN",c(3,4)],
      col=alpha("red",0.1),type="p",pch=19,cex=1.9)
lines(pca2vector[substr(fam.data$V2,1,3)=="EUR",c(3,4)],
      col=alpha("blue",0.1),type="p",pch=19,cex=1.9)
lines(pca2vector[substr(fam.data$V2,1,3)=="AMR",c(3,4)],
      col=alpha("green",0.1),type="p",pch=19,cex=1.9)
legend("bottomleft",legend=c(v.ethno,"AMR"),col=c("black","red","blue","green"),pch=19)
```

\newpage

# Blatt 5: Meta-Analyse und Annotation
```{r B5, echo=F,eval=T}
rm(list = setdiff(ls(), "plink_call"))
```
Termin: 14.01.2020

Abgabe: Meta-Analyse (Aufgabe 1 - Teil 2)

## Aufgabe 1: Meta-Analyse

Es liegen Summary Statistiken von sechs Studien zu einem Risikofaktor X vor, hier nur beispielhaft für einen Bereich von Chromosom 14. Alle Outputs wurden mittels [SNPTEST](https://mathgen.stats.ox.ac.uk/genetics_software/snptest/snptest.html#frequentist_tests) erzeugt. 

a) **Daten laden**: Öffnen Sie einen der Datensätze mit einem Text-Editor und machen Sie sich mit der Struktur vertraut. Laden Sie anschließend alle sechs Datensätze in R und schränken Sie die Datensätze auf die Schnittmenge der SNPs ein! Beantworten Sie folgende Fragen:

* Was ist der Phänotyp und auf welche Covariablen wurde adjustiert?

* Welches genetische Modell wurde verwendet und wie groß sind die Fallzahlen in den sechs Studien? 

* Wurden die Genotypen oder die Gendoses für die Analysen verwendet? 

* Ist das für alle Studien gleich? 

b) **Daten filtern**: Filtern Sie alle SNPs, für die in allen Studien ein Kommentar eingetragen ist (=Fehlermeldung). Hier konnte keine Statistik bestimmt werden (z.B. monomorphe SNPs). 

c) **Meta-Anaylse rechnen**: Berechnen Sie nun mit der Funktion *metagen* aus dem Paket **meta** die Meta-Statistiken. Dazu werden nur die beta-Schätzer und Standardfehler der einzelnen Studien gebraucht. Erstellen Sie eine geeignete Schleife und speichern Sie als Output bitte folgende Variablen:

* Beta-Schätzer, Standardfehler und p-Wert des fixed-effect Modells (FEM)

* Beta-Schätzer, Standardfehler und p-Wert des random-effect Modells (REM)

* Cochran's Q, p-Wert von Q, $I^2$, und Studienzahl als Gütemaße der Meta-Analyse

d) **QC-Check**: Erzeugen Sie je einen QQ- und Manhattan-Plot pro Modell. Nutzen Sie dazu die Funktionen *qq* und *manhattan* aus dem Paket **qqman**. Bestimmen Sie zusätzlich den Inflationsfaktor $\lambda$ pro Modell und interpretieren Sie diese. 

Hinweise: Um $\lambda$ auszurechnen müssen Sie die (1-p-Werte) in $\chi^2$-verteilte Teststatistiken $Y^2$ transformieren (z.B. mit *qchisq* mit einem Freiheitsgrad). Dann ist bei *n* SNPs

$$ \lambda = \frac{median(Y_1^2,...,Y_n^2)}{0.456}$$

Für *manhattan* müssen Sie noch das Chromosom ergänzen (kein Standard-Output von SNPTEST). Da hier nur eine Region von Chromosom 14 betrachtet wird, ist es auch hilfreich die x-Achse auf die gegebenen Basenpositionen zu beschränken. 

e) Ihr Projektleiter hat entschieden, dass Sie Ihre Analysen wiederholen sollen. Diesmal sollen aber nur die Studien 2, 3, 5, und 6 in Ihre Meta-Analyse eingeschlossen werden, da hier auch die Covariablen übereinstimmen. Wiederholen Sie die Meta-Analyse, bestimmmen Sie wieder die $\lambda$s und erstellen Sie neue Plots. Wie ändert sich die Inflationsfaktoren? Ist eine weitere Korrektur nötig, oder kann man die Daten nun auswerten?  

### Lösung a)

* pheno CORT --> Phänotyp Cortisol (Steroidhormon)

* cov_names sex age BMI in Study 1 & 4, cov_names age BMI in Study 2, 3, 5 & 6 --> Unterschiedliche Adjustierung, mögliche Fehlerquelle

* frequentist 1 --> additives Modell

* method expected --> Gendoses

|Studie|1|2|3|4|5|6|
|------|-|-|-|-|-|-|
|Fallzahl|5597|2943|2654|2070|1358|712|

```{r B5A1_1, eval=T, echo=T,cache=T,}
tab1<-read.table("../data/Study1.out",header=T,as.is=T)
tab2<-read.table("../data/Study2.out",header=T,as.is=T)
tab3<-read.table("../data/Study3.out",header=T,as.is=T)
tab4<-read.table("../data/Study4.out",header=T,as.is=T)
tab5<-read.table("../data/Study5.out",header=T,as.is=T)
tab6<-read.table("../data/Study6.out",header=T,as.is=T)

table(tab1$rsid==tab2$rsid)
table(tab1$rsid==tab3$rsid)
table(tab1$rsid==tab4$rsid)
table(tab1$rsid==tab5$rsid)
table(tab1$rsid==tab6$rsid)
table(tab4$rsid==tab5$rsid)
table(tab4$rsid==tab6$rsid)

filt1<-is.element(tab1$rsid,tab4$rsid)
tab1<-tab1[filt1,]
tab2<-tab2[filt1,]
tab3<-tab3[filt1,]

filt2<-is.element(tab4$rsid,tab1$rsid)
tab4<-tab4[filt2,]
tab5<-tab5[filt2,]
tab6<-tab6[filt2,]

table(tab1$rsid==tab2$rsid)
table(tab1$rsid==tab3$rsid)
table(tab1$rsid==tab4$rsid)
table(tab1$rsid==tab5$rsid)
table(tab1$rsid==tab6$rsid)
```

Insgesamt sind imm 156420 SNPs in den Studien, aber nur 156415 in der Schnittmenge. 

### Lösung b)

```{r B5A1_2, eval=T, echo=T,cache=T}
filt3<-!is.na(tab1$comment) & !is.na(tab2$comment) & !is.na(tab3$comment) & 
  !is.na(tab4$comment) & !is.na(tab5$comment) & !is.na(tab6$comment) 
table(filt3)
table(filt3,!is.na(tab1$comment))

tab1_1<-tab1[!filt3,]
tab2_1<-tab2[!filt3,]
tab3_1<-tab3[!filt3,]
tab4_1<-tab4[!filt3,]
tab5_1<-tab5[!filt3,]
tab6_1<-tab6[!filt3,]
```

Insgesamt sind 96543 SNPs immer NA. Diese werden gefiltert. Für die Meta-Analyse stehen daher 59872 SNPs zur Verfügung. 

### Lösung c)

```{r B5A1_3, eval=T, echo=T,cache=T}
# Variablen erstellen
beta_fix<-c()
se_fix<-c()
p_fix<-c()
beta_ran<-c()
se_ran<-c()
p_ran<-c()
k<-c()
Q<-c()
pQ <- c()
I2<-c()

# einfache for Schleife für alle SNPs
j<-seq(1,60000,10000)
for (i in 1:59872){
  # i=25973
  if(is.element(i,j)==T)message(paste0("working on SNP ", i))
  myBetas<-c(tab1_1$frequentist_add_beta_1[i],tab2_1$frequentist_add_beta_1[i],
             tab3_1$frequentist_add_beta_1[i],tab4_1$frequentist_add_beta_1[i],
             tab5_1$frequentist_add_beta_1[i],tab6_1$frequentist_add_beta_1[i])
  mySEs<-c(tab1_1$frequentist_add_se_1[i],tab2_1$frequentist_add_se_1[i],
           tab3_1$frequentist_add_se_1[i],tab4_1$frequentist_add_se_1[i],
           tab5_1$frequentist_add_se_1[i],tab6_1$frequentist_add_se_1[i])
  mod<-metagen(myBetas,mySEs,
               studlab = c("Study 1","Study 2","Study 3","Study 4","Study 5","Study 6"))
  #print(summary(mod))
  #forest(mod)
  beta_fix[i]<-mod$TE.fixed
  se_fix[i]<-mod$seTE.fixed
  p_fix[i]<-mod$pval.fixed
  beta_ran[i]<-mod$TE.random
  se_ran[i]<-mod$seTE.random
  p_ran[i]<-mod$pval.random
  k[i]<-mod$k
  Q[i]<-mod$Q
  pQ[i]<-mod$pval.Q
  I2[i]<-mod$I2
}
myTab<-data.frame(tab1_1[,2:6],beta_fix,se_fix,p_fix,beta_ran,se_ran,p_ran,Q,pQ,I2,k)
colnames(myTab)
```

### Lösung d)

```{r B5A1_4, eval=T, echo=T,cache=T}
# vorbereiten 
table(myTab$chromosome)
myTab$chromosome<-14

# lambda ausrechnen
Y1<-qchisq(1-myTab$p_fix,df = 1)
lambda1<-median(Y1)/0.456
Y2<-qchisq(1-myTab$p_ran,df = 1)
lambda2<-median(Y2)/0.456
lambda1;lambda2

# Plots FEM
qq(myTab$p_fix, main=paste("QQ-Plot FEM, vor QC, Inflation",round(lambda1,3),sep=": "))
manhattan(myTab, chr="chromosome", bp="position", p="p_fix",
          main = "Manhattan-Plot FEM, vor QC",
          suggestiveline = -log10(1e-06), genomewideline =-log10(5e-08),
          xlim=c(92574620,97479339))

# Plots REM
qq(myTab$p_ran, main=paste("QQ-Plot REM, vor QC, Inflation",round(lambda2,3),sep=": "))
manhattan(myTab, chr="chromosome", bp="position", p="p_ran",
          main = "Manhattan-Plot REM, vor QC",
          suggestiveline = -log10(1e-06), genomewideline =-log10(5e-08),
          xlim=c(92574620,97479339))
```

Kommentar zu $\lambda$:

Bei GWAS nimmt man an, dass die Teststatistiken näherungsweise standard-normalverteilt sind. Wenn die Nullhypothese zutrifft, haben die Teststatistiken daher eine Varianz von 1. Inflation ist eine Abweichung von der Nullhypothese, und die Varianz nimmt den Wert $\lambda$ an, wobei $\lambda>1$. Der Median von nicht zentrierten, $\chi^2$-verteilten Zufallsvariablen beträgt $0.456*\lambda$. Man vergleicht daher den Median seiner eigenen, beobachteten Teststatistiken gegenüber dem erwarteten Median. Wenn $\lambda$ etwa 1 ist, kann man davon ausgehen, dass die Nullhypothese nicht verletzt ist. Wenn $\lambda$ von 1 stark abweicht, kann man nicht mehr von einer Nullverteilung ausgehen, und muss dann geeignet adjustieren. Weiter Infos zur Bestimmung des Medians stehen bei [**Gross et al.**](https://doi.org/10.1186/s12863-017-0571-x) (Additional File 2, Kapitel 6). Deflationen treten nicht natürlich auf, sondern entstehen z.B. bei polygenetischen Phänotypen unter Verwandtschaftskorrektur: der Phänotyp hat eine hohe Heritabilität, die Effekte werden jedoch durch die Adjustierung rauskorrigiert. In diesem Fall sollte man sein Model überdenken. Inflation tritt bei Populationsstratifikation auf. Hier muss mittels $\lambda$ korrigiert oder das Model angepasst werden (z.B. die ersten PCs mit verwenden).  

### Lösung e)

Der Grund für die hohe Inflation war, dass die Samples der Studie 2 und 3 bzw. 5 und 6 Teilmenge der Studie 1 bzw. 4 ist: in 2 und 5 wurden nur die Männer, in 3 und 6 nur die Frauen analysiert (daher auch keine Adjustierung auf Geschlecht in diesen vier Studien). Da jedes Sample doppelt verwendet wird, ist Annahme von unabhängigen Studien verletzt. Die Verwendung von nur den Studien 2, 3, 5 und 6 sollte dieses Problem lösen. Alternativ hätte man auch nur Studie 1 und 4 verwenden können. 

```{r B5A1_5, eval=T, echo=T,cache=T}
# filtern
filt4<-!is.na(tab2$comment) & !is.na(tab3$comment) & 
  !is.na(tab5$comment) & !is.na(tab6$comment) 
table(filt4)
table(filt4,!is.na(tab2$comment))

tab2_2<-tab2[!filt4,]
tab3_2<-tab3[!filt4,]
tab5_2<-tab5[!filt4,]
tab6_2<-tab6[!filt4,]

# Meta-Analyse
beta_fix<-c()
se_fix<-c()
p_fix<-c()
beta_ran<-c()
se_ran<-c()
p_ran<-c()
k<-c()
Q<-c()
pQ <- c()
I2<-c()
for (i in 1:56497){
  # i=25973
  if(is.element(i,j)==T)message(paste0("working on SNP ", i))
  myBetas<-c(tab2_2$frequentist_add_beta_1[i],tab3_2$frequentist_add_beta_1[i],
             tab5_2$frequentist_add_beta_1[i],tab6_2$frequentist_add_beta_1[i])
  mySEs<-c(tab2_2$frequentist_add_se_1[i],tab3_2$frequentist_add_se_1[i],
           tab5_2$frequentist_add_se_1[i],tab6_2$frequentist_add_se_1[i])
  mod<-metagen(myBetas,mySEs,studlab = c("Study 2","Study 3","Study 5","Study 6"))
  #print(summary(mod))
  #forest(mod)
  beta_fix[i]<-mod$TE.fixed
  se_fix[i]<-mod$seTE.fixed
  p_fix[i]<-mod$pval.fixed
  beta_ran[i]<-mod$TE.random
  se_ran[i]<-mod$seTE.random
  p_ran[i]<-mod$pval.random
  k[i]<-mod$k
  Q[i]<-mod$Q
  pQ[i]<-mod$pval.Q
  I2[i]<-mod$I2
}
myTab2<-data.frame(tab2_2[,2:6],beta_fix,se_fix,p_fix,beta_ran,se_ran,p_ran,k,Q,pQ,I2)
colnames(myTab2)
myTab2$chromosome<-14

# lambda ausrechnen
Y3<-qchisq(1-myTab2$p_fix,df = 1)
lambda3<-median(Y3)/0.456
Y4<-qchisq(1-myTab2$p_ran,df = 1)
lambda4<-median(Y4)/0.456
lambda3;lambda4

# Plots FEM
qq(myTab2$p_fix, main=paste("QQ-Plot FEM, vor QC, Inflation",round(lambda3,3),sep=": "))
manhattan(myTab2, chr="chromosome", bp="position", p="p_fix",
          main = "Manhattan-Plot FEM, vor QC",
          suggestiveline = -log10(1e-06), genomewideline =-log10(5e-08),
          xlim=c(92574620,97479339))

# Plots REM
qq(myTab2$p_ran, main=paste("QQ-Plot REM, vor QC, Inflation",round(lambda4,3),sep=": "))
manhattan(myTab2, chr="chromosome", bp="position", p="p_ran",
          main = "Manhattan-Plot REM, vor QC",
          suggestiveline = -log10(1e-06), genomewideline =-log10(5e-08),
          xlim=c(92574620,97479339))

```

Nach Filterung auf Kommentare bleiben hier 56497 SNPs für die Meta-Analyse übrig. Der Inflationsfaktor $\lambda$ sollte idealerweise 1 sein. Aber da hier nur ein Abschnitt eines Chromosoms angeschaut wird, kann eine Deflation schon auftauchen. Die Daten können daher jetzt ausgewertet werden. 

\newpage

## Aufgabe 2: Filter für Top-SNPs

Die Meta-Daten müssen allerdings noch gefiltert werden. Welche Thresholds fallen Ihnen zu den folgenden Maßen ein: MAF, info, Q, $I^2$, k, p-Wert und $r^2$? Filtern Sie Ihre Meta-Ergebnisse. Nutzen Sie für MAF und info bitte die minimale MAF bzw. info der vier Studien. Erzeugen Sie einen Forest Plot zu dem bestassoziierten SNP. Dazu können Sie die Funktion *forest* aus dem **meta** Paket nutzen, sie müssen lediglich nochmal das Meta-Modell für diesen SNP bestimmen (copy-paste aus der Schleife mit fixem *i*). Interpretieren Sie den Forest-Plot!

### Lösung 

* MAF<0.01: minor allele frequency

* info<0.5: Imputationsqualität

* p(Q)<0.05: Cochrans Q Statistik, Heterogenitätsmaß, Summe der gewichteten Differenzen, p-Wert mit k-1 Freiheitsgraden

* $I^2$>75%: $I^2$ Statistik, Heterogenitätsmaß, Prozentsatz der Variation zwischen Studien

* k<2: Anzahl der Studien

* $p<5x10^-8$: p-Wert der Meta-Analyse (Standard = FEM)

* $r^2$>0.5: LD-Maß, statistische Abhängigkeit der Marker, der bestassoziierte SNP bleibt, SNPs in LD zu diesem werden gefiltert

```{r B5A2, eval=T, echo=T,cache=T}
# Vorbereitung
dumTab1<-data.frame(tab2_2$all_maf,tab3_2$all_maf,tab5_2$all_maf,tab6_2$all_maf)
myTab2$minMAF<-apply(dumTab1, 1, FUN=min)

dumTab2<-data.frame(tab2_2$info,tab3_2$info,tab5_2$info,tab6_2$info)
myTab2$minInfo<-apply(dumTab2, 1, FUN=min)

# Filtern
filt_maf<-myTab2$minMAF<0.01
filt_info<-myTab2$minInfo<0.5
filt_het<-myTab2$I2>0.75
filt_k <- myTab2$k<2
filt5<- filt_maf | filt_info | filt_het | filt_k
table(filt5)
myTab3<-myTab2[!filt5,]

# lambda ausrechnen
Y5<-qchisq(1-myTab3$p_fix,df = 1)
lambda5<-median(Y5)/0.456
Y6<-qchisq(1-myTab3$p_ran,df = 1)
lambda6<-median(Y6)/0.456
lambda5;lambda6

# Plots FEM
qq(myTab3$p_fix, main=paste("QQ-Plot FEM, nach QC, Inflation",round(lambda5,3),sep=": "))
manhattan(myTab3, chr="chromosome", bp="position", p="p_fix",
          main = "Manhattan-Plot FEM, nach QC",
          suggestiveline = -log10(1e-06), genomewideline =-log10(5e-08),
          xlim=c(92574620,97479339))

# Plots REM
qq(myTab3$p_ran, main=paste("QQ-Plot REM, nach QC, Inflation",round(lambda6,3),sep=": "))
manhattan(myTab3, chr="chromosome", bp="position", p="p_ran",
          main = "Manhattan-Plot REM, nach QC",
          suggestiveline = -log10(1e-06), genomewideline =-log10(5e-08),
          xlim=c(92574620,97479339))
```

```{r B5A3, fig.height = 3, fig.width = 5,fig.align = "center",cache=T}
# Forest Plot vom Tophit
ordering<-order(myTab3$p_fix,decreasing=F)

head(myTab3[ordering,])
x<-grep("rs9989237:",myTab2$rsid)
myBetas<-c(tab2_2$frequentist_add_beta_1[x],tab3_2$frequentist_add_beta_1[x],
             tab5_2$frequentist_add_beta_1[x],tab6_2$frequentist_add_beta_1[x])
mySEs<-c(tab2_2$frequentist_add_se_1[x],tab3_2$frequentist_add_se_1[x],
           tab5_2$frequentist_add_se_1[x],tab6_2$frequentist_add_se_1[x])
mod<-metagen(myBetas,mySEs,studlab = c("Study 2","Study 3","Study 5","Study 6"))
print(summary(mod))
forest(mod)

# SNP Liste für LD-Tool
filt6<-myTab3$p_fix<1e-6
dummy1<-unlist(strsplit(x = myTab3$rsid[filt6],split = ":"))
dummy1<-dummy1[grepl("rs",dummy1)]
dummy1

filt7<-myTab3$p_fix<5e-8
dummy2<-unlist(strsplit(x = myTab3$rsid[filt7],split = ":"))
dummy2<-dummy2[grepl("rs",dummy2)]
dummy2

write.table(dummy1,file="results_meta/SNPList_gw.txt",row.names = F,col.names = F,quote = F)
write.table(dummy2,file="results_meta/SNPList_sug.txt",row.names = F,col.names = F,quote = F)
```

Insgesamt bleiben 11 genomweit signifikante SNPs übrig. Um auf $r^2$ zu testen, kann ein Online-Tool verwendet werden (s. nächste Aufgabe).

Interpretation Forest Plot: einzelne Studieneffekte homogen (alle positiv). SNP ist in Studien 2, 3 und 5 signifikant assoziiert, in Studie 6 nicht (Konfidenzbereich beinhaltet die 0). Diese Studie hat auch die kleinste Fallzahl und damit kleinstes Gewicht (7.1%). Insgesamt gibt es einen signifikanten positiven SNP-Effekt auf Cortisol. 

## Aufgabe 3: Online-Annotation

a) Schlagen Sie den Top-SNP in [dbSNP](https://www.ncbi.nlm.nih.gov/snp/) nach! Stimmen die Allele und die MAF überein?

b) Welche Gene liegen in der Nähe? Ist davon eines plausibler als andere? Wenn ja, warum? Nutzen Sie dazu z.B. [GeneCards](https://www.genecards.org/). 

c) Was ist zu der Genexpression des Kandidatengens bzw. des Top-SNPs bekannt? Nutzen Sie dazu z.B. das [GTEx Portal](https://gtexportal.org/home/). 

d) Welche anderen Phänotypen sind mit diesem Gen bzw. Lokus assoziiert? Nutzen Sie dazu z.B. den [GWAS Catalog](https://www.ebi.ac.uk/gwas/home).

e) Wie schaut die LD-Struktur in der Region aus? Nutzen Sie dazu alle SNPs mit $p<1*10^{-5}$ und z.B. das Tool [LDmatrix](https://ldlink.nci.nih.gov/?tab=ldmatrix).

f) Testen Sie Online-Annotation wie z.B. [FUMA](https://fuma.ctglab.nl/) oder [SNiPA](https://snipa.helmholtz-muenchen.de/snipa3/). 

### Lösung

Die Allele simmen mit dem Datenbankeintrag überein, und die MAF ist vergleichbar. 

![Top SNP in dbSNP](results_meta/01_dbSNP.jpg)

\newpage

Der SNP ist upstream des Gen *SERPINA6*, könnte also im Promotorbereich davon liegen. Dieses Gen ist ein guter Kandidat, da es sich hier um das Corticosteroid-Binding Globulin handelt. Es bindet also Cortisol. Je mehr CBG, desto weniger freies Cortisol im Blut. Eine Mutation in diesem Gen kann daher das Cortisol-Level im Blut stark beeinflussen. 

![Gene in der Nähe des Top SNPs](results_meta/02_dbSNP.jpg)

\newpage

![Genannotation in GHR](results_meta/03_GHR.jpg)

\newpage

Das Gen *SERPINA6* ist vor allem in der Leben exprimiert. Das ist auch plausibel, da Cortisol aus Cholesterol erzeugt wird, und Cholesterol vor allem in der Leber produziert wird. 

![Expressionsprofil von SERPINA6](results_meta/04_GTEx.jpg)

\newpage

Im GWAS Catalog sind bislang folgende Phänotypen für *SERPINA6* bekannt: Blutprotein-Levels, Alpha-1-Antitrypsin Levels, BMI-adjustierte Waist-Hip-Ratio, FEV/FEC Ratio (Bronchodilatator-Antwort) und Triglyceridlevels. 

![Ergebnisse für SERPINA6 in GWAS Catalog](results_meta/05_GWASCatalog.jpg)

\newpage

Alle 11 genomweit signifikanten SNPs sind in einem LD-Plot (blau markiert in der Abbildung). Es gibt also nur ein unabhängiges Signal. 

![LD Matrix der SNPs, die mindestens mit -log(p)=5 assoziiert sind](results_meta/06_LDPlot.jpg)

\newpage

# Blatt 6: Varianten der Mendelsche Randomisierung (MR)

```{r B6, echo=F,eval=T}
rm(list = setdiff(ls(), "plink_call"))
```
Termin: 28.01.2020

keine Abgabe

![Gerichteter azyklischer Graph der Mendelschen Randomisierung](Blatt6_MR.jpg)

## Aufgabe 1: Ratio-Methode
In der ersten Aufgabe soll pro Instrument der kausale Schätzer $\beta_{IV}$ mittels der Ratio-Methode bestimmt werden. Schätzer und Standardfehler kann man wie folgt berechnen:

$$\hat{\beta}_{IV,Ratio} = \hat{\beta}_Y / \hat{\beta}_X$$

$$SE_1(\hat{\beta}_{IV,Ratio})= se(\hat{\beta}_Y) / \hat{\beta}_X$$

$$SE_2(\hat{\beta}_{IV,Ratio})= \sqrt{\frac{se(\hat{\beta}_Y)^2}{\hat{\beta}_X^2} + \frac{\hat{\beta}_Y^2se(\hat{\beta}_X)^2}{\hat{\beta}_X^4}}$$

a) Laden Sie den Datensatz **MR.RData** und machen Sie sich mit dem Aufbau vertraut. 

b) Testen Sie die Korrelation und Assoziation von *x* auf *y* und *y.bin*

c) Bestimmen sie folgende Parameter für alle SNPs *g1* - *g4*:

* Die Schätzer aus den jeweiligen linearen Regressionen: $\hat{\beta}_Y$, $se(\hat{\beta}_Y)$, $\hat{\beta}_X$, $se(\hat{\beta}_X)$

* Der kausale Schätzer $\beta_{IV}$ und beide Standardfehler $SE_1$ und $SE_2$ sowie die dazugehörigen P-Werte

* Die F-Statistik der Regression des Risikofaktors. 

* Die MAF

d) Bezogen auf den Standardfehler erster Ordnung, welche genetische Variante liefert das präziseste Ergebnis? Wodurch wird die Präzision beeinflusst? Wann und wo unterscheiden sich die Fehler erster und zweiter Ordnung am meisten?

e) Unterscheidet sich der kausale Schätzer von der beobachteten Assoziation? Welche kausalen Schätzer sind signifikant?


### Lösung a) 

```{r B6A1_1, echo=T,eval=T}
load("../data/MR.RData")
dim(myDat)
colnames(myDat)
table(myDat[,2])
attach(myDat)
```

Wir haben also Daten von 1000 Personen und 4 SNPs *g1* - *g4*, einem Risikofaktor *x* und zwei Outcomes *y* (kontinuierlich) und *y.bin* (binär). Die beiden kontiunierlichen Größen sind annähernd normalverteilt.

Die SNPs sind klassisch codiert, d.h. Genotyp AA entspricht 0, AB 1 und BB 2.

### Lösung b) 

```{r B6A1_2, echo=T,eval=T}
par(mfrow=c(2,2))
hist(x)
hist(y)
plot(x,y)
boxplot(x~y.bin)

cor.test(y,x)
summary(lm(y~x))

t.test(x~y.bin)
summary(glm(y.bin~x,family="binomial"))

t.test(y~y.bin)
summary(glm(y.bin~y,family="binomial"))
```

Der Risikofaktor ist nur schwach mit *y* korreliert. Da wir nichts über die Confounder von *x* und *y* wissen, kann es sein, das diese die Korrelation abschwächen:

* Wenn ein Confounder auf *x* und *y* die gleiche Effektrichtung hat, wird die Korrelation überschätzt. 

* Wenn ein Confounder auf *x* und *y* unterschiedliche Effektrichtungen hat, wird die Korrelation unterschätzt. 

### Lösung c) 

```{r B6A1_3, echo=T,eval=T}
ratio_estimate <- function(g,x,y){
  mody<-lm(y ~ g)
  modx<-lm(x ~ g)
  by<-summary(mody)$coef[2,1]
  byse<-summary(mody)$coef[2,2]
  bx<-summary(modx)$coef[2,1]
  bxse<-summary(modx)$coef[2,2]
  beta.ratio<-by/bx
  beta.ratio
  se.ratio.st<- byse/sqrt(bx^2)
  se.ratio.nd<- sqrt(byse^2/bx^2 + by^2*bxse^2/bx^4)
  p1<-2*pnorm(-abs(beta.ratio/se.ratio.st))
  p2<-2*pnorm(-abs(beta.ratio/se.ratio.nd))
  fstat<- summary(modx)$f[1]
  maf<- (sum(g==1) + 2*sum(g==2))/(2*length(g))   
  return(c(by,byse,bx,bxse, beta.ratio,se.ratio.st,se.ratio.nd,p1,p2,fstat,maf))
}

ratio_estimate(g=g1,x=x,y=y)

g_mat<-cbind(g1,g2,g3,g4)
ratio.all<-round(apply(g_mat,2,ratio_estimate,x=x,y=y),4)
row.names(ratio.all)<-c("by","byse","bx","bxse", "beta.ratio","se.ratio.st",
                         "se.ratio.nd","pval.st","pval.nd","fstat","maf")
ratio.all
```

### Lösung d) 

```{r B6A1_4, echo=T,eval=T}
# Präzision
grep(min(ratio.all[6,]),ratio.all[6,])

# Differenz SE1 vs SE2
ratio.all[6,]-ratio.all[7,]
```

Das Instrument *g2* liefert den kleinsten Standardfehler erster Ordnung. Ein Grund dafür ist die starke Assoziation von *g2* mit dem Risikofaktor (F=23.66). 

Die Standardfehler erster und zweiter Ordnung unterscheiden sich vor allem bei *g4*. Auch das beruht auf der Assoziation mit dem Risikofaktor: *g4* ist nicht mit *x* assoziiert (F=0.48).

### Lösung e)

Die beobachtete Assoziation beträgt -0.125, auch die Korrelation ist negativ (Pearsons r=-0.097). Jedoch ist der kausale Schätzer für 3 von 4 Instrumenten positiv. Das kann unter starken Confounder-Einfluss passieren. 

Von den vier Instrumenten liefert nur einer einen signifikanten Schätzer unter Verwendung von $SE_1$, nämlich *g3* (Signifikanzniveau 5%). 

## Aufgabe 2: Two-Stage least squares Methode (2SLS oder TSLS)

Bei dieser Methode wird der kausale Schätzer mittels zweifacher Regression bestimmt:

1. Stufe: Regression des Risikofaktors auf alle SNPs

2. Stufe: Regression des Outcome auf die gefitteten Werte des Risikofaktors aus der 1. Stufe


a) Führen Sie ein TSLS per Hand für alle SNPs gemeinsam durch und notieren Sie sich den Schätzer und dessen Standardfehler!

b) Nutzen Sie nun die ivreg Funktion des R-Pakets ivpack und führen Sie ebenfalls eine TSLS für alle SNPs gemeinsam durch! 

c) Wie unterscheiden sich die beiden Ergebnisse?
d) Wiederholen Sie b) für *g1* und vergleichen Sie das Ergebnis mit dem von Aufgabe 1!

Per Hand bezieht sich hier darauf, dass man beide Stufen per Hand rechnet, und nicht alles in einem Schritt wie mit der *ivreg* Funktion von **ivpack**.

### Lösung

```{r B6A2, echo=T,eval=T}
# per Hand
stage1<-lm(x~g1+g2+g3+g4)
fit<-stage1$fitted.values
stage2<-lm(y~fit)
summary(stage2)

# per ivpack
ivmod<-ivreg(y~x|g1+g2+g3+g4,x=T)
summary(ivmod)

# nur ein SNP
mod1<-ivreg(y~x|g1,x=T)
mod2<-ivreg(y~x|g2,x=T)
mod3<-ivreg(y~x|g3,x=T)
mod4<-ivreg(y~x|g4,x=T)
beta.2sls<-c(summary(mod1)$coef[2,1],summary(mod2)$coef[2,1],
             summary(mod3)$coef[2,1],summary(mod4)$coef[2,1])
se.2sls<-c(summary(mod1)$coef[2,2],summary(mod2)$coef[2,2],
           summary(mod3)$coef[2,2],summary(mod4)$coef[2,2])
pval.2sls<-c(summary(mod1)$coef[2,4],summary(mod2)$coef[2,4],
             summary(mod3)$coef[2,4],summary(mod4)$coef[2,4])
ratio.all<-rbind(ratio.all,beta.2sls,se.2sls,pval.2sls)

# Vergleich der kausalen Schaetzer
ratio.all[5,]-ratio.all[12,]
# Vergleich der Standardfehler
ratio.all[6,]-ratio.all[13,]
ratio.all[7,]-ratio.all[13,]

```

**Vergleich 2SLS per Hand - ivreg**:

Die beiden kausalen Schätzer sind die gleichen, aber die Standardfehler unterscheiden sich. Das kommt daher, dass *ivreg* die Unsicherheit der ersten Stufe mitberücksichtigt und die *per Hand* Methode nicht. 

**Vergleich Ratio - 2SLS**:
Die kausalen Schätzer sind quasi gleich, aber auch hier gibt es einen Unterschied in der Schätzung der Standardfehler. Bei der Ratio-Methode wird der Term erster bzw. zweiter Ordnung aus einer Delta-Methoden-Erweiterung für den Standardfehler eines Ratio-Schätzers verwendet. Da hier nach dem ersten bzw. zweiten Term aufgehört wird, ist der Standardfehler der Ratio-Methode bei starken Instrumenten immer kleiner als der der 2SLS-Methode. 

## Aufgabe 3: Inverse Varianz gewichtete Methode (inverse-variance weighted, IVW)

Oft hat man nur Summarized Data, d.h. nur die $\beta$s und $SE$s der einzelnen SNPs. Damit lässt sich keine TSLS durchführen. Stattdessen kann man den kausalen Schätzer mittels IVW bestimmen: 

$$\hat\beta_{IV,IVW} = \frac{\sum{\hat{\beta}_Y}\hat{\beta}_Xse(\hat{\beta}_Y)^{-2}}{\sum{\hat{\beta}_X^2se(\hat{\beta}_Y)^{-2}}}$$

$$SE_3(\hat\beta_{IV,IVW})= \sqrt{\frac{1}{\sum{\hat{\beta}_X^2se(\hat{\beta}_Y)^{-2}}}}$$

Dies entspricht einer Meta-Analyse der kausalen Schätzer (FEM). 

Bestimmen Sie den kausalen Meta-Effekt mittels jeweils mit und ohne SNP *g4*: 

a) der oben aufgeführten Funktionen

b) der Funktion *metagen* aus dem Paket **meta** (s. letzte R-Übung)

c) der Funktion *mr_allmethods* aus dem Paket **MendelianRandomization**

d) Gibt es Unterschiede bei den Kausalschätzungen? 

e) Erstellen Sie einen Scatterplot der vier Instrumente inkl. der Fehlerbalken (Hinweis: *mr_plot* aus **MendelianRandomization**)!

### Lösung a)

```{r B6A3_1, echo=T,eval=T}
# mit der aufgeführten Funktion
ratio.all2<-as.data.frame(t(ratio.all))
attach(ratio.all2)
beta.ivw<-sum(bx*by*byse^-2)/sum(bx^2*byse^-2)
se.ivw<-1/sqrt(sum(bx^2*byse^-2))
beta.ivw; se.ivw; 2*pnorm(-abs(beta.ivw/se.ivw))

# ohne SNP 4
ratio.all3<-ratio.all2[-4,]
attach(ratio.all3)
beta.ivw<-sum(bx*by*byse^-2)/sum(bx^2*byse^-2)
se.ivw<-1/sqrt(sum(bx^2*byse^-2))
beta.ivw; se.ivw; 2*pnorm(-abs(beta.ivw/se.ivw))
```

Das Ergebnis ist sehr ähnlich. Das liegt daran, dass *g4* nicht die erste MR-Bedingung erfüllt (starke Assoziation von G auf X)! Die IVW ist eine relativ robuste Methode, es können bis zu 50% der verwendeten Instrumente schwach sein, ohne das Ergebnis zu verfälschen. Allerdings sollte man bei bekannten Übertritt der Bedingung 1 besser filtern. 

### Lösung b)

```{r B6A3_2, echo=T,eval=T}
# mit meta 
myBetas<-ratio.all2$beta.ratio
mySEs1<-ratio.all2$se.ratio.st
mySEs2<-ratio.all2$se.ratio.nd
mod1<-metagen(myBetas,mySEs1,studlab = c("SNP 1","SNP 2","SNP 3","SNP 4"))
summary(mod1)
mod2<-metagen(myBetas,mySEs2,studlab = c("SNP 1","SNP 2","SNP 3","SNP 4"))
summary(mod2)

# ohne SNP 4
myBetas<-ratio.all2$beta.ratio[1:3]
mySEs1<-ratio.all2$se.ratio.st[1:3]
mySEs2<-ratio.all2$se.ratio.nd[1:3]
mod1<-metagen(myBetas,mySEs1,studlab = c("SNP 1","SNP 2","SNP 3"))
summary(mod1)
mod2<-metagen(myBetas,mySEs2,studlab = c("SNP 1","SNP 2","SNP 3"))
summary(mod2)
```

Die Berechnung mit $SE_1(\hat{\beta}_{IV,Ratio})$ liefert das gleiche Ergebnis wie die vorherige Methode. 

### Lösung c) - e)

```{r B6A3_3, echo=T,eval=T}
# mit MendelianRandomization
attach(ratio.all2)
MRObject<- mr_input(bx=ratio.all2$bx,bxse=ratio.all2$bxse, 
                    by=ratio.all2$by,byse = ratio.all2$byse)
tab1<-mr_allmethods(MRObject)
knitr::kable(tab1@Values,
             caption = "Ergebnis alle Meta-MR Methoden, die im Paket implementiert sind 
             (alle SNPs, Aufgabe 3, Blatt 6)",
             col.names = c("Method","Estimate","Std Error","lower 95% CI","upper 95% CI","P-value"))
mr_plot(MRObject,interactive = F,labels = T)
mr_plot(mr_allmethods(MRObject,method = "main"))

attach(ratio.all3)
MRObject<- mr_input(bx=ratio.all3$bx,bxse=ratio.all3$bxse, 
                    by=ratio.all3$by,byse = ratio.all3$byse)
tab2<-mr_allmethods(MRObject)
knitr::kable(tab2@Values,
             caption = "Ergebnis alle Meta-MR Methoden, die im Paket implementiert sind 
             (ohne SNP 4, Aufgabe 3, Blatt 6)",
             col.names = c("Method","Estimate","Std Error","lower 95% CI","upper 95% CI","P-value"))
mr_plot(MRObject,interactive = F,labels = T)
mr_plot(mr_allmethods(MRObject,method = "main"))

```

Die kausalen Schätzer sind insgesamt sehr ähnlich, aber ihre Signifikanz unterscheidet sich sehr. 

## Zusatz: MR mit binären Outcome

Natürlich kann man das Prinzip der MR auch auf binäre Phänotypen anwenden. Hierbei ist zu beachten, dass empfohlen wird, die G-X Assoziation nur auf den Kontrollen bzgl. Y zu rechnen. 

Führen Sie die Ratio-Methode bzw. TSLS für *g2* bzw. *g1 - g3* durch!

### Lösung

```{r B6Z_1, echo=T,eval=T}
attach(myDat)

# Ratio Methode
mody<-glm(y.bin ~ g2, family="binomial")
modx<-lm(x~g2,subset = y.bin==0)
by2.bin<-summary(mody)$coef[2,1]
byse2.bin<-summary(mody)$coef[2,2]
bx2.bin<-summary(modx)$coef[2,1]
ratio2.bin<-by2.bin/bx2.bin
se.ratio2.bin<-byse2.bin/bx2.bin
ratio2.bin; se.ratio2.bin; 2*pnorm(-abs(ratio2.bin/se.ratio2.bin))

# TSLS Methode
g1.con<-g1[y.bin==0]
g2.con<-g2[y.bin==0]
g3.con<-g3[y.bin==0]
x.con<-x[y.bin==0]
predict.con.g2<-predict(lm(x.con~g2.con),newdata=list(g2.con=g2))
predict.con<-predict(lm(x.con~g1.con+g2.con+g3.con),
                     newdata=c(list(g1.con=g1),list(g2.con=g2),list(g3.con=g3)))
tsls2.con<-glm(y.bin ~ predict.con.g2, family="binomial")
summary(tsls2.con)
tsls.con<-glm(y.bin ~ predict.con, family="binomial")
summary(tsls.con)
```

Die berechneten Schätzer repräsentieren die log-kausalen Odds Ratios für *y.bin* pro Einheitssteigerung von *x*. Zurückrechnen zu normalen OR erfolgt über die Exponential-Funktion, die Konfidenzintervalle kann man mittels Normal-Approximation bestimmen: 

```{r B6Z_2, echo=T,eval=T}
beta.tsls.con<-summary(tsls.con)$coef[2,1]
se.tsls.con<-summary(tsls.con)$coef[2,2]

lower.bound<-beta.tsls.con - 1.96*se.tsls.con
upper.bound<-beta.tsls.con + 1.96*se.tsls.con

OR<-exp(beta.tsls.con)
exp(lower.bound); OR; exp(upper.bound)
```

